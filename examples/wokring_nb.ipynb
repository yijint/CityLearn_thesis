{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing RBC, MPC, and SAC in CityLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xVzk4V7qUu2R"
   },
   "outputs": [],
   "source": [
    "# System operations\n",
    "import inspect\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Date and time\n",
    "from datetime import datetime\n",
    "\n",
    "# type hinting\n",
    "from typing import Any, List, Mapping, Tuple, Union\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# User interaction\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import Button, FloatSlider, HBox, HTML\n",
    "from ipywidgets import IntProgress, Text, VBox\n",
    "\n",
    "# Data manipulation\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import simplejson as json\n",
    "\n",
    "# CityLearn\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction\n",
    "from citylearn.wrappers import NormalizedObservationWrapper\n",
    "from citylearn.wrappers import StableBaselines3Wrapper\n",
    "from citylearn.wrappers import TabularQLearningWrapper\n",
    "\n",
    "# baseline RL algorithms\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set settings for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lDAe5ZeVw7_q"
   },
   "outputs": [],
   "source": [
    "# set all plotted figures without margins\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gRR9HOBxOR2"
   },
   "source": [
    "Load data from CityLearn 2021 Challenge. \n",
    "\n",
    "In this challenge, the agents must control the energy stored by a micro-grid of 9 buildings in real-time for a period of 4 simulated years, on an hourly time-scale. Each building has up to 3 action-variables (domestic hot water storage, chilled water storage, and electrical storage). This is a coordination challenge, so most of objectives of this control problem depend on the joint actions of all the buildings combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9SuxbmkixQ2z"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2021'\n",
    "schema = DataSet.get_schema(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root_directory': '/home/jt9744/.conda/envs/citylearn/lib/python3.9/site-packages/citylearn/data/citylearn_challenge_2021',\n",
       " 'central_agent': True,\n",
       " 'simulation_start_time_step': 0,\n",
       " 'simulation_end_time_step': 17520,\n",
       " 'episode_time_steps': None,\n",
       " 'rolling_episode_split': False,\n",
       " 'random_episode_split': False,\n",
       " 'seconds_per_time_step': 3600.0,\n",
       " 'observations': {'month': {'active': True, 'shared_in_central_agent': True},\n",
       "  'day_type': {'active': True, 'shared_in_central_agent': True},\n",
       "  'hour': {'active': True, 'shared_in_central_agent': True},\n",
       "  'daylight_savings_status': {'active': False,\n",
       "   'shared_in_central_agent': True},\n",
       "  'outdoor_dry_bulb_temperature': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'outdoor_dry_bulb_temperature_predicted_6h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'outdoor_dry_bulb_temperature_predicted_12h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'outdoor_dry_bulb_temperature_predicted_24h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'outdoor_relative_humidity': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'outdoor_relative_humidity_predicted_6h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'outdoor_relative_humidity_predicted_12h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'outdoor_relative_humidity_predicted_24h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'diffuse_solar_irradiance': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'diffuse_solar_irradiance_predicted_6h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'diffuse_solar_irradiance_predicted_12h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'diffuse_solar_irradiance_predicted_24h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'direct_solar_irradiance': {'active': True, 'shared_in_central_agent': True},\n",
       "  'direct_solar_irradiance_predicted_6h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'direct_solar_irradiance_predicted_12h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'direct_solar_irradiance_predicted_24h': {'active': True,\n",
       "   'shared_in_central_agent': True},\n",
       "  'carbon_intensity': {'active': True, 'shared_in_central_agent': True},\n",
       "  'indoor_dry_bulb_temperature': {'active': True,\n",
       "   'shared_in_central_agent': False},\n",
       "  'average_unmet_cooling_setpoint_difference': {'active': False,\n",
       "   'shared_in_central_agent': False},\n",
       "  'indoor_relative_humidity': {'active': True,\n",
       "   'shared_in_central_agent': False},\n",
       "  'non_shiftable_load': {'active': True, 'shared_in_central_agent': False},\n",
       "  'solar_generation': {'active': True, 'shared_in_central_agent': False},\n",
       "  'cooling_storage_soc': {'active': True, 'shared_in_central_agent': False},\n",
       "  'heating_storage_soc': {'active': False, 'shared_in_central_agent': False},\n",
       "  'dhw_storage_soc': {'active': True, 'shared_in_central_agent': False},\n",
       "  'electrical_storage_soc': {'active': True, 'shared_in_central_agent': False},\n",
       "  'net_electricity_consumption': {'active': True,\n",
       "   'shared_in_central_agent': False},\n",
       "  'electricity_pricing': {'active': False, 'shared_in_central_agent': False},\n",
       "  'electricity_pricing_predicted_6h': {'active': False,\n",
       "   'shared_in_central_agent': False},\n",
       "  'electricity_pricing_predicted_12h': {'active': False,\n",
       "   'shared_in_central_agent': False},\n",
       "  'electricity_pricing_predicted_24h': {'active': False,\n",
       "   'shared_in_central_agent': False},\n",
       "  'power_outage': {'active': False, 'shared_in_central_agent': False}},\n",
       " 'actions': {'cooling_storage': {'active': True},\n",
       "  'heating_storage': {'active': False},\n",
       "  'dhw_storage': {'active': True},\n",
       "  'electrical_storage': {'active': True}},\n",
       " 'agent': {'type': 'citylearn.agents.sac.SAC',\n",
       "  'attributes': {'hidden_dimension': [256, 256],\n",
       "   'discount': 0.99,\n",
       "   'tau': 0.005,\n",
       "   'lr': 0.003,\n",
       "   'batch_size': 256,\n",
       "   'replay_buffer_capacity': 100000.0,\n",
       "   'standardize_start_time_step': 6000,\n",
       "   'end_exploration_time_step': 7000,\n",
       "   'action_scaling_coef': 0.5,\n",
       "   'reward_scaling': 5.0,\n",
       "   'update_per_time_step': 2}},\n",
       " 'reward_function': {'type': 'citylearn.reward_function.IndependentSACReward',\n",
       "  'attributes': None},\n",
       " 'buildings': {'Building_1': {'include': False,\n",
       "   'energy_simulation': 'Building_1.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': [],\n",
       "   'inactive_actions': [],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.2,\n",
       "     'target_cooling_temperature': 8.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.9}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 2.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'dhw_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 2.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.008}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 140.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 75.0,\n",
       "     'power_efficiency_curve': [[0, 0.83],\n",
       "      [0.3, 0.83],\n",
       "      [0.7, 0.9],\n",
       "      [0.8, 0.9],\n",
       "      [1, 0.85]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 1], [1.0, 0.2]]}},\n",
       "   'pv': {'type': 'citylearn.energy_model.PV',\n",
       "    'autosize': False,\n",
       "    'attributes': {'nominal_power': 120.0}}},\n",
       "  'Building_2': {'include': True,\n",
       "   'energy_simulation': 'Building_2.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': ['solar_generation'],\n",
       "   'inactive_actions': [],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.21,\n",
       "     'target_cooling_temperature': 9.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.92}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 3.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'dhw_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 3.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.008}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 80.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 40.0,\n",
       "     'power_efficiency_curve': [[0, 0.8],\n",
       "      [0.3, 0.85],\n",
       "      [0.7, 0.92],\n",
       "      [0.8, 0.91],\n",
       "      [1, 0.82]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 0.8], [1.0, 0.23]]}}},\n",
       "  'Building_3': {'include': False,\n",
       "   'energy_simulation': 'Building_3.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': ['dhw_storage_soc', 'solar_generation'],\n",
       "   'inactive_actions': ['dhw_storage'],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.23,\n",
       "     'target_cooling_temperature': 8.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.87}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 2.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 50.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 20.0,\n",
       "     'power_efficiency_curve': [[0, 0.83],\n",
       "      [0.3, 0.83],\n",
       "      [0.7, 0.9],\n",
       "      [0.8, 0.9],\n",
       "      [1, 0.85]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 0.9], [1.0, 0.27]]}}},\n",
       "  'Building_4': {'include': False,\n",
       "   'energy_simulation': 'Building_4.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': ['dhw_storage_soc'],\n",
       "   'inactive_actions': ['dhw_storage'],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.22,\n",
       "     'target_cooling_temperature': 9.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.9}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 1.5},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 75.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 30.0,\n",
       "     'power_efficiency_curve': [[0, 0.83],\n",
       "      [0.3, 0.83],\n",
       "      [0.7, 0.9],\n",
       "      [0.8, 0.9],\n",
       "      [1, 0.85]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 0.95], [1.0, 0.2]]}},\n",
       "   'pv': {'type': 'citylearn.energy_model.PV',\n",
       "    'autosize': False,\n",
       "    'attributes': {'nominal_power': 40.0}}},\n",
       "  'Building_5': {'include': False,\n",
       "   'energy_simulation': 'Building_5.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': [],\n",
       "   'inactive_actions': [],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.24,\n",
       "     'target_cooling_temperature': 8.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.9}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 3.5},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'dhw_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 1.5},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.008}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 50.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 25.0,\n",
       "     'power_efficiency_curve': [[0, 0.8],\n",
       "      [0.3, 0.83],\n",
       "      [0.7, 0.87],\n",
       "      [0.8, 0.85],\n",
       "      [1, 0.8]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 0.83], [1.0, 0.35]]}},\n",
       "   'pv': {'type': 'citylearn.energy_model.PV',\n",
       "    'autosize': False,\n",
       "    'attributes': {'nominal_power': 25.0}}},\n",
       "  'Building_6': {'include': True,\n",
       "   'energy_simulation': 'Building_6.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': [],\n",
       "   'inactive_actions': [],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.2,\n",
       "     'target_cooling_temperature': 9.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.85}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 1.5},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'dhw_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 3.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.008}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 30.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 10.0,\n",
       "     'power_efficiency_curve': [[0, 0.83],\n",
       "      [0.3, 0.83],\n",
       "      [0.7, 0.9],\n",
       "      [0.8, 0.9],\n",
       "      [1, 0.85]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 1], [1.0, 0.2]]}},\n",
       "   'pv': {'type': 'citylearn.energy_model.PV',\n",
       "    'autosize': False,\n",
       "    'attributes': {'nominal_power': 20.0}}},\n",
       "  'Building_7': {'include': False,\n",
       "   'energy_simulation': 'Building_7.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': ['solar_generation'],\n",
       "   'inactive_actions': [],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.22,\n",
       "     'target_cooling_temperature': 8.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.9}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 2.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'dhw_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 2.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.008}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 40.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 15.0,\n",
       "     'power_efficiency_curve': [[0, 0.83],\n",
       "      [0.3, 0.83],\n",
       "      [0.7, 0.9],\n",
       "      [0.8, 0.9],\n",
       "      [1, 0.85]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 0.85], [1.0, 0.25]]}}},\n",
       "  'Building_8': {'include': False,\n",
       "   'energy_simulation': 'Building_8.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': ['solar_generation'],\n",
       "   'inactive_actions': [],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.24,\n",
       "     'target_cooling_temperature': 9.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.93}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 3.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'dhw_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 3.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.008}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 30.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 10.0,\n",
       "     'power_efficiency_curve': [[0, 0.8],\n",
       "      [0.3, 0.85],\n",
       "      [0.7, 0.9],\n",
       "      [0.8, 0.9],\n",
       "      [1, 0.85]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 1], [1.0, 0.3]]}}},\n",
       "  'Building_9': {'include': True,\n",
       "   'energy_simulation': 'Building_9.csv',\n",
       "   'weather': 'weather.csv',\n",
       "   'carbon_intensity': 'carbon_intensity.csv',\n",
       "   'pricing': None,\n",
       "   'inactive_observations': ['solar_generation'],\n",
       "   'inactive_actions': [],\n",
       "   'cooling_device': {'type': 'citylearn.energy_model.HeatPump',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None,\n",
       "     'efficiency': 0.22,\n",
       "     'target_cooling_temperature': 8.0,\n",
       "     'target_heating_temperature': 45.0}},\n",
       "   'dhw_device': {'type': 'citylearn.energy_model.ElectricHeater',\n",
       "    'autosize': True,\n",
       "    'attributes': {'nominal_power': None, 'efficiency': 0.9}},\n",
       "   'cooling_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 3.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.006}},\n",
       "   'dhw_storage': {'type': 'citylearn.energy_model.StorageTank',\n",
       "    'autosize': True,\n",
       "    'autosize_attributes': {'safety_factor': 3.0},\n",
       "    'attributes': {'capacity': None, 'loss_coefficient': 0.008}},\n",
       "   'electrical_storage': {'type': 'citylearn.energy_model.Battery',\n",
       "    'autosize': False,\n",
       "    'attributes': {'capacity': 35.0,\n",
       "     'efficiency': 0.9,\n",
       "     'capacity_loss_coefficient': 1e-05,\n",
       "     'loss_coefficient': 0.0,\n",
       "     'nominal_power': 20.0,\n",
       "     'power_efficiency_curve': [[0, 0.83],\n",
       "      [0.3, 0.83],\n",
       "      [0.7, 0.9],\n",
       "      [0.8, 0.9],\n",
       "      [1, 0.85]],\n",
       "     'capacity_power_curve': [[0.0, 1], [0.8, 1], [1.0, 0.3]]}}}}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoF-BxSM5Jkc"
   },
   "source": [
    "Data Preprocessing:\n",
    "1. Use only 3, 5, 7, or 9 buildings \n",
    "2. Use the first two years of data for training, the third year for validation, and the fourth year for testing.\n",
    "3. Use centralized control\n",
    "\n",
    "These can be modified directly in the schema. The buildings will be pseudo-randomly selected with a seed for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2o6iEzE_zP_U"
   },
   "outputs": [],
   "source": [
    "def set_schema_buildings(schema, scale='tiny', seed=2025) -> Tuple[dict, List[str]]:\n",
    "    \"\"\"Randomly select number of buildings to set as active in the schema.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    scale: str\n",
    "        Number of buildings to set as active in schema. 'tiny' = 3 buildings,\n",
    "        'small' = 5 buildings, 'medium' = 7 buildings, 'large' = 9 buildings.\n",
    "    seed: int\n",
    "        Seed for pseudo-random number generator\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with active buildings set.\n",
    "    buildings: List[str]\n",
    "        List of selected buildings.\n",
    "    \"\"\"\n",
    "    scales = ['tiny','small','medium','large']\n",
    "    assert scale in scales, f'scale must be one of {scales}'\n",
    "    if scale == 'tiny':\n",
    "        count = 3\n",
    "    elif scale =='small':\n",
    "        count = 5\n",
    "    elif scale == 'medium':\n",
    "        count = 7\n",
    "    elif scale == 'large':\n",
    "        count = 9 \n",
    "    assert 1 <= count <= len(schema['buildings']), f\"count must be between 1 and {len(schema['buildings'])}.\"\n",
    "\n",
    "    # set random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # get all building names\n",
    "    buildings = list(schema['buildings'].keys())\n",
    "\n",
    "    # randomly select specified number of buildings\n",
    "    buildings = np.random.choice(buildings, size=count, replace=False).tolist()\n",
    "\n",
    "    # reorder buildings\n",
    "    building_ids = [int(b.split('_')[-1]) for b in buildings]\n",
    "    building_ids = sorted(building_ids)\n",
    "    buildings = [f'Building_{i}' for i in building_ids]\n",
    "\n",
    "    # update schema to only included selected buildings\n",
    "    for b in schema['buildings']:\n",
    "        if b in buildings:\n",
    "            schema['buildings'][b]['include'] = True\n",
    "        else:\n",
    "            schema['buildings'][b]['include'] = False\n",
    "\n",
    "    return schema, buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TcfYt2eH0sP8"
   },
   "outputs": [],
   "source": [
    "def set_schema_simulation_period(schema, data_split) -> Tuple[dict, int, int]:\n",
    "    \"\"\"Select environment simulation start and end time steps based on the data split\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    data_split: str\n",
    "        'train', 'val', or 'test'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with `simulation_start_time_step`\n",
    "        and `simulation_end_time_step` key-values set.\n",
    "    simulation_start_time_step: int\n",
    "        The first time step in schema time series files to\n",
    "        be read when constructing the environment.\n",
    "    simulation_end_time_step: int\n",
    "        The last time step in schema time series files to\n",
    "        be read when constructing the environment.\n",
    "    \"\"\"\n",
    "    assert data_split in ['train', 'val', 'test'], f\"data_split must be one of {['train', 'val', 'test']}\"\n",
    "\n",
    "    # use any of the files to determine the total\n",
    "    # number of available time steps\n",
    "    filename = schema['buildings']['Building_1']['carbon_intensity']\n",
    "    filepath = os.path.join(schema['root_directory'], filename)\n",
    "    time_steps = pd.read_csv(filepath).shape[0]\n",
    "\n",
    "    # select a simulation start and end time step\n",
    "    if data_split == 'train':\n",
    "        simulation_start_time_step = 0\n",
    "        simulation_end_time_step = 2*365*24\n",
    "    elif data_split == 'val': \n",
    "        simulation_start_time_step = 2*365*24\n",
    "        simulation_end_time_step = 3*365*24\n",
    "    elif data_split == 'test':\n",
    "        simulation_start_time_step = 3*365*24\n",
    "        simulation_end_time_step = time_steps-1\n",
    "\n",
    "    # update schema simulation time steps\n",
    "    schema['simulation_start_time_step'] = simulation_start_time_step\n",
    "    schema['simulation_end_time_step'] = simulation_end_time_step\n",
    "\n",
    "    return schema, simulation_start_time_step, simulation_end_time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX5rxfNdz3lE"
   },
   "source": [
    "Setting the Buildings and Time Periods to use in Simulations from the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "6C6S46xmz50t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected buildings: ['Building_2', 'Building_6', 'Building_9']\n",
      "Selected train split: (0, 17520)\n",
      "Active observations: ['hour']\n"
     ]
    }
   ],
   "source": [
    "# edit next code line to change number of buildings in simulation\n",
    "scale = 'tiny'\n",
    "\n",
    " # edit next code line to change number of days in simulation\n",
    "split = 'train'\n",
    "\n",
    "# edit next code line to change active observations in simulation\n",
    "ACTIVE_OBSERVATIONS = ['hour']\n",
    "\n",
    "schema, buildings = set_schema_buildings(schema, scale)\n",
    "schema, simulation_start_time_step, simulation_end_time_step =\\\n",
    "    set_schema_simulation_period(schema, split)\n",
    "\n",
    "print('Selected buildings:', buildings)\n",
    "print(\n",
    "    f'Selected {split} split:',\n",
    "    (simulation_start_time_step, simulation_end_time_step)\n",
    ")\n",
    "print(f'Active observations:', ACTIVE_OBSERVATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xL8ayLYz90X"
   },
   "source": [
    "The choice between either control strategy is set using the `central_agent` parameter in CityLearn, which is a key-value in the `schema`. Use the centralized control strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Qh5FKi6Nopbr"
   },
   "outputs": [],
   "source": [
    "schema['central_agent'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSt6h_Q-oqjK"
   },
   "source": [
    "Initialize a CityLearn Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0aBJ5aLZosk-"
   },
   "outputs": [],
   "source": [
    "env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co9Zh6dk0UIt"
   },
   "source": [
    "Check initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "QYT8ouXg0WeH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time step: 0\n",
      "environment number of time steps: 17521\n",
      "environment uses central agent: True\n",
      "Number of buildings: 3\n"
     ]
    }
   ],
   "source": [
    "print('Current time step:', env.time_step)\n",
    "print('environment number of time steps:', env.time_steps)\n",
    "print('environment uses central agent:', env.central_agent)\n",
    "print('Number of buildings:', len(env.buildings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "gT64404w0dlu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electrical storage capacity: {'Building_2': 80.0, 'Building_6': 30.0, 'Building_9': 35.0}\n",
      "Electrical storage loss_coefficient: {'Building_2': 0.0, 'Building_6': 0.0, 'Building_9': 0.0}\n",
      "Electrical storage initial_soc: {'Building_2': 0.0, 'Building_6': 0.0, 'Building_9': 0.0}\n",
      "Electrical storage efficiency: {'Building_2': 0.9, 'Building_6': 0.9, 'Building_9': 0.9}\n",
      "Electrical storage electricity consumption: {'Building_2': 0.0, 'Building_6': 0.0, 'Building_9': 0.0}\n",
      "\n",
      "PV nominal power: {'Building_2': 0.0, 'Building_6': 20.0, 'Building_9': 0.0}\n",
      "\n",
      "Active observations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building</th>\n",
       "      <th>month</th>\n",
       "      <th>day_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>daylight_savings_status</th>\n",
       "      <th>outdoor_dry_bulb_temperature</th>\n",
       "      <th>outdoor_dry_bulb_temperature_predicted_6h</th>\n",
       "      <th>outdoor_dry_bulb_temperature_predicted_12h</th>\n",
       "      <th>outdoor_dry_bulb_temperature_predicted_24h</th>\n",
       "      <th>outdoor_relative_humidity</th>\n",
       "      <th>outdoor_relative_humidity_predicted_6h</th>\n",
       "      <th>outdoor_relative_humidity_predicted_12h</th>\n",
       "      <th>outdoor_relative_humidity_predicted_24h</th>\n",
       "      <th>diffuse_solar_irradiance</th>\n",
       "      <th>diffuse_solar_irradiance_predicted_6h</th>\n",
       "      <th>diffuse_solar_irradiance_predicted_12h</th>\n",
       "      <th>diffuse_solar_irradiance_predicted_24h</th>\n",
       "      <th>direct_solar_irradiance</th>\n",
       "      <th>direct_solar_irradiance_predicted_6h</th>\n",
       "      <th>direct_solar_irradiance_predicted_12h</th>\n",
       "      <th>direct_solar_irradiance_predicted_24h</th>\n",
       "      <th>carbon_intensity</th>\n",
       "      <th>indoor_dry_bulb_temperature</th>\n",
       "      <th>average_unmet_cooling_setpoint_difference</th>\n",
       "      <th>indoor_relative_humidity</th>\n",
       "      <th>non_shiftable_load</th>\n",
       "      <th>solar_generation</th>\n",
       "      <th>cooling_storage_soc</th>\n",
       "      <th>heating_storage_soc</th>\n",
       "      <th>dhw_storage_soc</th>\n",
       "      <th>electrical_storage_soc</th>\n",
       "      <th>net_electricity_consumption</th>\n",
       "      <th>electricity_pricing</th>\n",
       "      <th>electricity_pricing_predicted_6h</th>\n",
       "      <th>electricity_pricing_predicted_12h</th>\n",
       "      <th>electricity_pricing_predicted_24h</th>\n",
       "      <th>power_outage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Building_2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Building_6</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Building_9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     building  month  day_type  hour  daylight_savings_status  \\\n",
       "0  Building_2   True      True  True                    False   \n",
       "1  Building_6   True      True  True                    False   \n",
       "2  Building_9   True      True  True                    False   \n",
       "\n",
       "   outdoor_dry_bulb_temperature  outdoor_dry_bulb_temperature_predicted_6h  \\\n",
       "0                          True                                       True   \n",
       "1                          True                                       True   \n",
       "2                          True                                       True   \n",
       "\n",
       "   outdoor_dry_bulb_temperature_predicted_12h  \\\n",
       "0                                        True   \n",
       "1                                        True   \n",
       "2                                        True   \n",
       "\n",
       "   outdoor_dry_bulb_temperature_predicted_24h  outdoor_relative_humidity  \\\n",
       "0                                        True                       True   \n",
       "1                                        True                       True   \n",
       "2                                        True                       True   \n",
       "\n",
       "   outdoor_relative_humidity_predicted_6h  \\\n",
       "0                                    True   \n",
       "1                                    True   \n",
       "2                                    True   \n",
       "\n",
       "   outdoor_relative_humidity_predicted_12h  \\\n",
       "0                                     True   \n",
       "1                                     True   \n",
       "2                                     True   \n",
       "\n",
       "   outdoor_relative_humidity_predicted_24h  diffuse_solar_irradiance  \\\n",
       "0                                     True                      True   \n",
       "1                                     True                      True   \n",
       "2                                     True                      True   \n",
       "\n",
       "   diffuse_solar_irradiance_predicted_6h  \\\n",
       "0                                   True   \n",
       "1                                   True   \n",
       "2                                   True   \n",
       "\n",
       "   diffuse_solar_irradiance_predicted_12h  \\\n",
       "0                                    True   \n",
       "1                                    True   \n",
       "2                                    True   \n",
       "\n",
       "   diffuse_solar_irradiance_predicted_24h  direct_solar_irradiance  \\\n",
       "0                                    True                     True   \n",
       "1                                    True                     True   \n",
       "2                                    True                     True   \n",
       "\n",
       "   direct_solar_irradiance_predicted_6h  \\\n",
       "0                                  True   \n",
       "1                                  True   \n",
       "2                                  True   \n",
       "\n",
       "   direct_solar_irradiance_predicted_12h  \\\n",
       "0                                   True   \n",
       "1                                   True   \n",
       "2                                   True   \n",
       "\n",
       "   direct_solar_irradiance_predicted_24h  carbon_intensity  \\\n",
       "0                                   True              True   \n",
       "1                                   True              True   \n",
       "2                                   True              True   \n",
       "\n",
       "   indoor_dry_bulb_temperature  average_unmet_cooling_setpoint_difference  \\\n",
       "0                         True                                      False   \n",
       "1                         True                                      False   \n",
       "2                         True                                      False   \n",
       "\n",
       "   indoor_relative_humidity  non_shiftable_load  solar_generation  \\\n",
       "0                      True                True             False   \n",
       "1                      True                True              True   \n",
       "2                      True                True             False   \n",
       "\n",
       "   cooling_storage_soc  heating_storage_soc  dhw_storage_soc  \\\n",
       "0                 True                False             True   \n",
       "1                 True                False             True   \n",
       "2                 True                False             True   \n",
       "\n",
       "   electrical_storage_soc  net_electricity_consumption  electricity_pricing  \\\n",
       "0                    True                         True                False   \n",
       "1                    True                         True                False   \n",
       "2                    True                         True                False   \n",
       "\n",
       "   electricity_pricing_predicted_6h  electricity_pricing_predicted_12h  \\\n",
       "0                             False                              False   \n",
       "1                             False                              False   \n",
       "2                             False                              False   \n",
       "\n",
       "   electricity_pricing_predicted_24h  power_outage  \n",
       "0                              False         False  \n",
       "1                              False         False  \n",
       "2                              False         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Active actions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building</th>\n",
       "      <th>cooling_storage</th>\n",
       "      <th>heating_storage</th>\n",
       "      <th>dhw_storage</th>\n",
       "      <th>electrical_storage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Building_2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Building_6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Building_9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     building  cooling_storage  heating_storage  dhw_storage  \\\n",
       "0  Building_2             True            False         True   \n",
       "1  Building_6             True            False         True   \n",
       "2  Building_9             True            False         True   \n",
       "\n",
       "   electrical_storage  \n",
       "0                True  \n",
       "1                True  \n",
       "2                True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# electrical storage\n",
    "print('Electrical storage capacity:', {\n",
    "    b.name: b.electrical_storage.capacity for b in env.buildings\n",
    "})\n",
    "print('Electrical storage loss_coefficient:', {\n",
    "    b.name: b.electrical_storage.loss_coefficient for b in env.buildings\n",
    "})\n",
    "print('Electrical storage initial_soc:', {\n",
    "    b.name: b.electrical_storage.initial_soc for b in env.buildings\n",
    "})\n",
    "print('Electrical storage efficiency:', {\n",
    "    b.name: b.electrical_storage.efficiency for b in env.buildings\n",
    "})\n",
    "print('Electrical storage electricity consumption:', {\n",
    "    b.name: b.electrical_storage.electricity_consumption[b.time_step]\n",
    "    for b in env.buildings\n",
    "})\n",
    "print()\n",
    "\n",
    "# pv\n",
    "print('PV nominal power:', {\n",
    "    b.name: b.pv.nominal_power for b in env.buildings\n",
    "})\n",
    "print()\n",
    "\n",
    "# active observations and actions\n",
    "with pd.option_context(\n",
    "    'display.max_rows', None,\n",
    "    'display.max_columns', None,\n",
    "    'display.width', None\n",
    "):\n",
    "    print('Active observations:')\n",
    "    display(pd.DataFrame([\n",
    "        {**{'building':b.name}, **b.observation_metadata}\n",
    "        for b in env.buildings\n",
    "    ]))\n",
    "    print()\n",
    "    print('Active actions:')\n",
    "    display(pd.DataFrame([\n",
    "        {**{'building':b.name}, **b.action_metadata}\n",
    "        for b in env.buildings\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2jH6nki0hpG"
   },
   "source": [
    "Given a set of GEBs, the controllers aim to minimize the electricity cost, average peak demand, and carbon intensity of electricity consumed by the buildings. The evalu\n",
    "tion will be based on these metrics while also including computational costs suchas \n",
    " training time, inference time, RAM usage, and scalability across different numbe off\r\n",
    " builgs.\n",
    "\n",
    "Three built-in key performance indicators can be used: cost, carbon emissions, and average daily peak. Average daily peak is a district-level KPI that is calculated using the aggregated district-level hourly net electricity consumption (kWh), $E_h^{\\textrm{district}}$. Cost and carbon emissions are building-level KPIs that are calculated using the building-level hourly net electricity consumption (kWh), $E_h^{\\textrm{building}}$, and are reported at the grid level as the average of the building-level values.\n",
    "\n",
    "Cost is defined as the sum of building-level imported electricity cost, $E_h^{\\textrm{building}} \\times T_h$ (\\$), where $T_h$ is the electricity rate at hour $h$.\n",
    "\n",
    "$$\n",
    "    \\textrm{cost} = \\sum_{h=0}^{n-1}{\\textrm{max} \\left (0,E_h^{\\textrm{building}} \\times T_h \\right )}\n",
    "$$\n",
    "\n",
    "Carbon emissions is the sum of building-level carbon emissions (kg<sub>CO<sub>2</sub>e</sub>), $E_h^{\\textrm{building}} \\times O_h$, where $O_h$ is the carbon intensity (kg<sub>CO<sub>2</sub>e</sub>/kWh) at hour $h$.\n",
    "\n",
    "$$\n",
    "    \\textrm{carbon emissions} = \\sum_{h=0}^{n-1}{\\textrm{max} \\left (0,E_h^{\\textrm{building}} \\times O_h \\right )}\n",
    "$$\n",
    "\n",
    "Average daily peak, is defined as the mean of the daily $E_h^{\\textrm{district}}$ peak where $d$ is the day index and $n$ is the total number of days.\n",
    "\n",
    "$$\n",
    "    \\textrm{average daily peak} = \\frac{\n",
    "        {\\sum}_{d=0}^{n - 1} {\\sum}_{h=0}^{23} {\\textrm{max} \\left (E_{24d + h}^{\\textrm{district}}, \\dots, E_{24d + 23}^{\\textrm{district}} \\right)}\n",
    "    }{n}\n",
    "$$\n",
    "\n",
    "The KPIs are reported as normalized values with respect to the baseline outcome where the baseline outcome is when buildings are not equipped with batteries i.e., no control.\n",
    "\n",
    "$$\\textrm{KPI} = \\frac{{\\textrm{KPI}_{control}}}{\\textrm{KPI}_{baseline (no\\ battery)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa9Iuu2GU52Z"
   },
   "source": [
    "Helper functions to parse and illustrate the KPIs from CityLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "aG2SsVYq0sye"
   },
   "outputs": [],
   "source": [
    "from kpi_utils import get_kpis, plot_building_kpis, plot_district_kpis\n",
    "from kpi_utils import plot_building_load_profiles, plot_district_load_profiles, plot_battery_soc_profiles, plot_simulation_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWXsiZ5freTG"
   },
   "source": [
    "# Build your Custom Rule-Based Controller\n",
    "---\n",
    "\n",
    "With our convenience functions defined, we are ready to start solving our earlier described control problem.\n",
    "\n",
    "We will start simple with a rule-based control (RBC) agent that you will build yourself! RBC is a popular control strategy that is used in most systems e.g. HVAC, batteries, etc because of their level of simplicity. They are  best described as a set of rules expressed as if-else statements and conditions that guide their decision making. An example of such statement is `if outdoor dry-bulb temperature is 20 degrees Celcius and hour 10 PM, charge battery with 5% of capacity`. Now the actual implementation of this statement is open-ended as a designer can choose to program it using any programming language e.g. Python (as used in CityLearn) or a proprietary language that the battery manufacturer uses. Nevertheless, at a high-level, it simplifies to a set of statements and conditions that are easily understood and mappable (think decision tree in supervised learning).\n",
    "\n",
    "The RBC you will be designing here, is a set of if-else statements that use the `hour` observation to determine the amount of energy to charge or discharge a battery. Remember we are using a centralized control strategy thus, the if-else statements you define will apply to all batteries in all buildings.\n",
    "\n",
    "We will use widgets for an interactive RBC tuning experience. You will design a custom RBC that inherits from an existing RBC in CityLearn called the [HourRBC](https://www.citylearn.net/api/citylearn.agents.rbc.html#citylearn.agents.rbc.HourRBC). Inheritance, allows us to copy existing properties and methods in the parent class, `HourRBC`, into our custom class. The `HourRBC` class allows one to define a custom `action_map` using the `hour` as the `if-else` condition and the battery capacity proportion as the `action` where negative proportions imply discharging and positive proportions imply charging.\n",
    "\n",
    "We begin by initializing the environment we will work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "unHiw8HH1bzD"
   },
   "outputs": [],
   "source": [
    "rbc_env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXyGFcYE1d5j"
   },
   "source": [
    "Now let us define the custom RBC class we will use. All agent classes in CityLearn inherit from the [citylearn.agents.base.Agent](https://www.citylearn.net/api/citylearn.agents.base.html#citylearn.agents.base.Agent) class. This base class has 4 methods that are important to note when defining a new class that inherits from it. namely:\n",
    "\n",
    "1. `__init__` - Used to initialize a new agent with a `citylearn.citylearn.CityLearnEnv` object.\n",
    "2. `learn`: Used to train the initialized object on its environment object.\n",
    "3. `predict`: Used to select actions at each simulation timestep using a defined policy that may be rule-based, reinforcement learning-based or model predictive control-based. The base class selects random actions.\n",
    "4. `update`: Used to update replay buffers, networks and policies at least every timestep. The base class does not perform any updates.\n",
    "5. `next_time_step`: Used to proceed to the next timestep and is called inside `predict`. This function is where class values or custom values that need to collected or updated are best manipulated.\n",
    "\n",
    "In our case with the RBC, we want to include an `action_map` class instance that is a `dict` type. This `action_map` has `int` keys that define hours and `float` values that define charge/discharge action for the hour key that maps them.\n",
    "\n",
    "We also want to include a loader variable to help us visualize the simulation progress. The loader is an `IntProgress` ipywidgets object. We will update the loader's value each timestep the `next_time_step` method is called in the RBC class.\n",
    "\n",
    "The RBC class is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "M-qNSqYSuiY-"
   },
   "outputs": [],
   "source": [
    "class CustomRBC(HourRBC):\n",
    "   def __init__(\n",
    "       self, env: CityLearnEnv, action_map: Mapping[int, float] = None,\n",
    "       loader: IntProgress = None\n",
    "    ):\n",
    "      r\"\"\"Initialize CustomRBC.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      env: Mapping[str, CityLearnEnv]\n",
    "         CityLearn environment instance.\n",
    "      action_map: Mapping[int, float]\n",
    "         Mapping of hour to control action.\n",
    "      loader: IntProgress\n",
    "         Progress bar.\n",
    "      \"\"\"\n",
    "\n",
    "      super().__init__(env=env, action_map=action_map)\n",
    "      self.loader = loader\n",
    "\n",
    "   def next_time_step(self):\n",
    "      r\"\"\"Advance to next `time_step`.\"\"\"\n",
    "\n",
    "      super().next_time_step()\n",
    "\n",
    "      if self.loader is not None:\n",
    "         self.loader.value += 1\n",
    "      else:\n",
    "         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFj5ovew1uqG"
   },
   "source": [
    "We can now initialize the RBC by setting all actions to 0 for every hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2cr_3VaD1wbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default RBC action map: {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0}\n"
     ]
    }
   ],
   "source": [
    "action_map = {i: 0.0 for i in range(1, 25)}\n",
    "rbc_model = CustomRBC(env=rbc_env, action_map=action_map)\n",
    "print('default RBC action map:', action_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubLZV4k4H0H2"
   },
   "source": [
    "We also need to define a convenience function to set and return a loader i.e. a progress bar as we will use this visualization a number of times to track our learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ST3YhSkRIDLT"
   },
   "outputs": [],
   "source": [
    "def get_loader(**kwargs):\n",
    "    \"\"\"Returns a progress bar\"\"\"\n",
    "\n",
    "    kwargs = {\n",
    "        'value': 0,\n",
    "        'min': 0,\n",
    "        'max': 10,\n",
    "        'description': 'Simulating:',\n",
    "        'bar_style': '',\n",
    "        'style': {'bar_color': 'maroon'},\n",
    "        'orientation': 'horizontal',\n",
    "        **kwargs\n",
    "    }\n",
    "    return IntProgress(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIOhohXk12Vo"
   },
   "source": [
    "With our custom RBC now defined, we can set up the interactive widgets.\n",
    "\n",
    ">  **NOTE**:\n",
    "> You do not need to understand the content of the next code cell where the widget is defined. Instead wait for the widgets to load and interact with it using the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wdmxDMHJuiY_"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x150 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4915a031c2f04fd19e744aad4d5ff0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<h1>Custom RBC Tuner</h1>\\n<p>Use this interactive widget to tune your custom RBC!\\nReference th"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c22d40e5a714b1eacd70ee78761664d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatSlider(value=0.0, continuous_update=False, description='Hr: 1-2', max=1.0, min=-1.0, orien"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddba78d657914779bc7f65e19117d35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='info', description='Reset', style=ButtonStyle(), tooltip='Set all hour act"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310411bd507b456dbedbac6e98dc0cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Waiting', max=10, style=ProgressStyle(bar_color='maroon'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "action_step = 0.05\n",
    "hour_step = 2\n",
    "hours = list(range(1, 25, hour_step))\n",
    "default_loader_description = 'Waiting'\n",
    "questions = \"\"\"\n",
    "<h1>Custom RBC Tuner</h1>\n",
    "<p>Use this interactive widget to tune your custom RBC!\n",
    "Reference the building load profiles above and the questions below when\n",
    "deciding on how to charge/discharge your rule-based controlled batteries.</p>\n",
    "\n",
    "<h3>Some considerations when tuning your custom RBC:</h3>\n",
    "<ul>\n",
    "    <li>What happens when actions for all hours are set to 0?</li>\n",
    "    <li>How can we set the RBC so that it takes advantage\n",
    "    of solar generation?</li>\n",
    "    <li>Can you spot the duck curve?</li>\n",
    "    <li>What settings work best for a specific building?</li>\n",
    "    <li>What settings work best for the entire district?</li>\n",
    "    <li>Can you tune the RBC to target improvements in any one of\n",
    "    the evaluation KPIs?</li>\n",
    "    <li>What challenges can you identify from this RBC tuning process?</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Interact with the controls to tune your RBC:</h3>\n",
    "\n",
    "<p>Use the sliders to set the hourly charge and discharge rate\n",
    "of the batteries. Positive values indicate charging\n",
    "and negative values indicate discharging the batteries</p>\n",
    "\"\"\"\n",
    "html_ui = HTML(value=questions, placeholder='Questions')\n",
    "sliders = [FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-1.0,\n",
    "    max=1.0,\n",
    "    step=action_step,\n",
    "    description=f'Hr: {h}-{h + hour_step - 1}',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='vertical',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ") for h in hours]\n",
    "reset_button = Button(\n",
    "    description='Reset', disabled=False, button_style='info',\n",
    "    tooltip='Set all hour actions to 0.0', icon=''\n",
    ")\n",
    "random_button = Button(\n",
    "    description='Random', disabled=False, button_style='warning',\n",
    "    tooltip='Select random hour actions', icon=''\n",
    ")\n",
    "simulate_button = Button(\n",
    "    description='Simulate', disabled=False, button_style='success',\n",
    "    tooltip='Run simulation', icon='check'\n",
    ")\n",
    "sliders_ui = HBox(sliders)\n",
    "buttons_ui = HBox([reset_button, random_button, simulate_button])\n",
    "\n",
    "# run simulation so that the environment has results\n",
    "# even if user does not interact with widgets\n",
    "sac_episodes = 1\n",
    "rbc_model.learn(episodes=sac_episodes)\n",
    "\n",
    "loader = get_loader(description=default_loader_description)\n",
    "\n",
    "def plot_building_guide(env):\n",
    "    \"\"\"Plots building load and generation profiles.\"\"\"\n",
    "\n",
    "    column_count_limit = 4\n",
    "    building_count = len(env.buildings)\n",
    "    row_count = math.ceil(building_count/column_count_limit)\n",
    "    column_count = min(column_count_limit, building_count)\n",
    "    figsize = (4.0*column_count, 1.5*row_count)\n",
    "    fig, _ = plt.subplots(row_count, column_count, figsize=figsize)\n",
    "\n",
    "    for i, (ax, b) in enumerate(zip(fig.axes, env.buildings)):\n",
    "        y1 = b.energy_simulation.non_shiftable_load\n",
    "        y2 = b.pv.get_generation(b.energy_simulation.solar_generation)\n",
    "        x = range(len(y1))\n",
    "        ax.plot(x, y1, label='Load')\n",
    "        ax.plot(x, y2, label='Generation')\n",
    "        ax.set_title(b.name)\n",
    "        ax.set_xlabel('Time step')\n",
    "        ax.set_ylabel('kWh')\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "\n",
    "        if i == building_count - 1:\n",
    "            ax.legend(\n",
    "                loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0\n",
    "            )\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def on_reset_button_clicked(b):\n",
    "    \"\"\"Zeros sliders and loader values.\"\"\"\n",
    "\n",
    "    loader.value = 0\n",
    "    loader.description = default_loader_description\n",
    "\n",
    "    for s in sliders:\n",
    "        s.value = 0.0\n",
    "\n",
    "def on_random_button_clicked(b):\n",
    "    \"\"\"Zeros loader value and sets sliders to random values.\"\"\"\n",
    "\n",
    "    loader.value = 0\n",
    "    loader.description = default_loader_description\n",
    "    options = np.arange(-1.0, 1.0, action_step)\n",
    "\n",
    "    for s in sliders:\n",
    "        s.value = round(random.choice(options), 2)\n",
    "\n",
    "def on_simulate_button_clicked(b):\n",
    "    \"\"\"Runs RBC simulation using selected action map.\"\"\"\n",
    "\n",
    "    loader.description = 'Simulating'\n",
    "    loader.value = 0\n",
    "    clear_output(wait=False)\n",
    "\n",
    "    # plot building profiles\n",
    "    _ = plot_building_guide(rbc_env)\n",
    "    plt.show()\n",
    "\n",
    "    display(html_ui, sliders_ui, buttons_ui, loader)\n",
    "    reset_button.disabled = True\n",
    "    random_button.disabled = True\n",
    "    simulate_button.disabled = True\n",
    "\n",
    "    for s in sliders:\n",
    "        s.disabled = True\n",
    "\n",
    "    action_map = {}\n",
    "\n",
    "    for h, s in zip(hours, sliders):\n",
    "        for i in range(hour_step):\n",
    "            action_map[h + i] = s.value\n",
    "\n",
    "    loader.max = rbc_env.time_steps*sac_episodes\n",
    "    rbc_model.action_map = action_map\n",
    "    rbc_model.learn(episodes=sac_episodes)\n",
    "\n",
    "    loader.description = 'Finished'\n",
    "    plot_simulation_summary({'RBC': rbc_env})\n",
    "\n",
    "    reset_button.disabled = False\n",
    "    random_button.disabled = False\n",
    "    simulate_button.disabled = False\n",
    "\n",
    "    for s in sliders:\n",
    "        s.disabled = False\n",
    "\n",
    "reset_button.on_click(on_reset_button_clicked)\n",
    "random_button.on_click(on_random_button_clicked)\n",
    "simulate_button.on_click(on_simulate_button_clicked)\n",
    "\n",
    "# plot building profiles\n",
    "_ = plot_building_guide(rbc_env)\n",
    "plt.show()\n",
    "\n",
    "# preview of building load profile\n",
    "display(html_ui, sliders_ui, buttons_ui, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KHS2--GuiY_"
   },
   "source": [
    "# An Introduction to Tabular Q-Learning Algorithm as an Adaptive Controller\n",
    "---\n",
    "\n",
    "Tuning your RBC must have revealed that it is a cumbersome and labor intensive process, especially as the number of buildings, time period and variance in load profiles increase. What we will be ideal is an adaptive controller that can adjust to different occupant preferences and behaviors in each building that influence load profiles and adjust to different weather conditions that affect the seasonal variance in load profiles.\n",
    "\n",
    "Moreover, we want a controller that is able to learn with little to no knowledge about the environment model it is controlling unlike the RBC tuning process where you probably chose your charge and discharge proportion by visually inspecting the building load and generation profiles. Instead, we want a controller that can learn those patterns in a data-driven fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8nW-IMQ2Oah"
   },
   "source": [
    "## Q-Learning Background\n",
    "[Tabular Q-Learning](https://link.springer.com/article/10.1007/BF00992698) is a popular model-free reinforcement learning technique due to its simplicity. In simple tasks with small finite state sets, and discrete actions, all transitions can be represented using a table, hence the name Tabular Q-Learning, which stores the state-action values, i.e., Q-values.\n",
    "\n",
    "After taking an action $a$, given a state $s$, and observing the immediate reward $r$ for taking $a$ at $s$, learning is achieved through updating $Q(s, a)$ ([Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation)) as:\n",
    "\n",
    "$$\n",
    "Q(s, a) = Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]\n",
    "$$\n",
    "\n",
    "where $Q(s, a)$ is the Q-value for taking action $a$ in state $s$, $\\alpha  [0, 1]$ is the learning rate, which explicitly defines to what degree new knowledge overrides old knowledge: for $\\alpha = 0$, no learning happens, while for $\\alpha = 1$, all prior knowledge is lost. $\\gamma$ is the discount factor which allow to balance between an agent that considers only immediate rewards ($\\gamma$ = 0) and one that strives towards long term rewards ($\\gamma$ = 1). $\\max_{a'} Q(s', a')$ is the maximum Q-value for all actions $a'$ in the next state $s'$ that is reached after taking action $a$ in state $s$.\n",
    "\n",
    "In other words, the optimal policy, $\\pi$, results from taking those actions $a$ that maximize the respective Q-values in each state, $s$. In order for the algorithm to converge to the optimal policy, the requirement is that each state-action pair $(s, a)$ be visited infinitely many times, such that the Q-values have converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mtY8kPK2RgT"
   },
   "source": [
    "### Algorithm\n",
    "\n",
    "The general Q-Learning algorithm is as follows:\n",
    "\n",
    "> 1. Initialize the Q-table for all state-action pairs.\n",
    "> 2. Set the learning rate $\\alpha$ ($0 < \\alpha < 1$) and the discount factor $\\gamma$ ($0 < \\gamma < 1$).\n",
    "> 3. Repeat the following steps for each episode:\n",
    ">     - Observe the initial state $s$.\n",
    ">     - Choose an action $a$ based on the epsilon-greedy policy (a random action is chosen with probability epsilon, $\\epsilon$ and the action with the highest Q-value is chosen with probability $1 - \\epsilon$).\n",
    ">     - Take the action $a$ and observe the next state $s'$ and the reward $r$.\n",
    ">     - Update the Q-value of the state-action pair $(s,a)$ using the Bellman equation.\n",
    ">     - Set $s = s'$.\n",
    "> 4. Repeat step 3 for a large number of episodes or until convergence is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI_mZrJ-2WXJ"
   },
   "source": [
    "### Action Selection\n",
    "\n",
    "In Q-learning, the process of accumulating knowledge happens through the trade-off between exploiting known, high-reward, actions, and exploring other, unknown, actions that have not been executed yet under that state. The $\\epsilon$-greedy approach which we use here, selects a random action with probability epsilon, $\\epsilon$ (exploration), and the action with the highest expected return with probability $1 - \\epsilon$ (exploitation). This balancing allows the agent to avoid local minima (exploration), while striving towards convergence (exploitation). In practice, $\\epsilon$ is set relatively large in the beginning of the learning process, and then reduced progressively. The choice of the initial value and the reduction strategy is domain specific and task of the designer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JCbSW0t2aYa"
   },
   "source": [
    "## CityLearn Tabular Q-Learning Implementation\n",
    "\n",
    "CityLearn has a Tabular Q-learning implementation in its `citylearn.agents.q_learning.TabularQLearning` class (see [docs](https://www.citylearn.net/api/citylearn.agents.base.html#citylearn.agents.q_learning.TabularQLearning)). This Q-Learning implementation is inspired by the [BOPTEST Tutorial](https://colab.research.google.com/drive/1WeA_3PQeySba0MMRRte_oZTF7ptlP_Ra#scrollTo=9U81QUVcUfoW&line=17&uniqifier=1) but follows the general algorithm in the literature that we have described. However, a caveat of making use of this agent is that it requires discrete observations and actions in order to update the Q-Table whereas the default CityLearn environment provides continuous observations and actions.\n",
    "\n",
    "CityLearn provides an environment wrapper, `TabularQLearningWrapper` (see [docs](https://www.citylearn.net/api/citylearn.wrappers.html#citylearn.wrappers.DiscreteSpaceWrapper)) used to discretize observations and actions before passing to an agent. All we need to do is define the number of bins to use to discretize the observations and actions using the wrapper's `observation_bin_sizes` and `action_bin_sizes` initialization variables.\n",
    "\n",
    "We begin by initializing a new environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "_j2GAa7YuiZA"
   },
   "outputs": [],
   "source": [
    "tql_env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8L28axZ4NVj"
   },
   "source": [
    "We will discretize the hour into 24 bins and the action into 12 bins. Hour is an observation shared by all buildings thus, its values are the same in all buildings at each time step. For this reason, one of the dimensions of our Q-Table will equal the hour bin count. The action space for controlling the batteries has the same size as number of buildings thus when discretized, the other Q-Table dimension will equal the `electrical_storage` action raised to the power of building count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_6HotiSW4Pe8"
   },
   "outputs": [],
   "source": [
    "# define active observations and actions and their bin sizes\n",
    "observation_bins = {'hour': 24}\n",
    "action_bins = {'electrical_storage': 12}\n",
    "\n",
    "# initialize list of bin sizes where each building\n",
    "# has a dictionary in the list definining its bin sizes\n",
    "observation_bin_sizes = []\n",
    "action_bin_sizes = []\n",
    "\n",
    "for b in tql_env.buildings:\n",
    "    # add a bin size definition for the buildings\n",
    "    observation_bin_sizes.append(observation_bins)\n",
    "    action_bin_sizes.append(action_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFBOCDVU2nGT"
   },
   "source": [
    "Can you think of a way to choose more appropriate bin sizes? How does the choice of bin size affect the learning process?\n",
    "\n",
    "Now we wrap the environment to make sure we are exchanging discrete observations and actions between the environment and agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "1rkt9jnNuiZE"
   },
   "outputs": [],
   "source": [
    "tql_env = TabularQLearningWrapper(\n",
    "    tql_env.unwrapped,\n",
    "    observation_bin_sizes=observation_bin_sizes,\n",
    "    action_bin_sizes=action_bin_sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqiqVp0b21zG"
   },
   "source": [
    "We can now go ahead to initialize our Q Learner. We will modify the CityLearn `TabularQLearning` class like we did the `HourRBC` so that we are able to visually track the learning process as well as keep tabs on its cummulative reward as training episodes go by. We also provide a `random_seed` instance variable that we set to the random seed you defined earlier. This random seed will ensure that each time this notebook is run, the epsilon-greedy action selections are reproducible. The modifications to the `TabularQLearning` class are done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5s9klu5nuiZF"
   },
   "outputs": [],
   "source": [
    "class CustomTabularQLearning(TabularQLearning):\n",
    "    def __init__(\n",
    "        self, env: CityLearnEnv, loader: IntProgress,\n",
    "        random_seed: int = None, **kwargs\n",
    "    ):\n",
    "        r\"\"\"Initialize CustomRBC.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: Mapping[str, CityLearnEnv]\n",
    "            CityLearn environment instance.\n",
    "        loader: IntProgress\n",
    "            Progress bar.\n",
    "        random_seed: int\n",
    "            Random number generator reprocucibility seed for\n",
    "            eqsilon-greedy action selection.\n",
    "        kwargs: dict\n",
    "            Parent class hyperparameters\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(env=env, random_seed=random_seed, **kwargs)\n",
    "        self.loader = loader\n",
    "        self.reward_history = []\n",
    "\n",
    "    def next_time_step(self):\n",
    "        if self.env.time_step == 0:\n",
    "            self.reward_history.append(0)\n",
    "\n",
    "        else:\n",
    "            self.reward_history[-1] += sum(self.env.rewards[-1])\n",
    "\n",
    "        self.loader.value += 1\n",
    "        super().next_time_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXHQVD_427-s"
   },
   "source": [
    "With our Tabular Q-Learning agent set up, it is time to train it on our environment. We will use the following hyperparameters:\n",
    "\n",
    "- `epsilon` ($\\epsilon$) = 1.0\n",
    "- `minimum_epsilon` ($\\epsilon_{\\textrm{min}}$) = 0.01\n",
    "- `epsilon_decay` ($\\epsilon_{\\textrm{decay}}$) = 0.0001\n",
    "- `discount_factor` ($\\gamma$) = 0.99\n",
    "- `learning_rate` ($\\alpha$) = 0.005\n",
    "\n",
    "The agent is trained for $\\frac{m \\times n \\times i}{t}$ episodes where $m$ and $n$ are the observation and action space sizes respectively, $i$ is an arbitrary integer and t is the number of time steps in one episode. That way, we increase the probability that we at least visit each state-action combination once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "uS7RzQUyuiZF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table dimension: (24, 144)\n",
      "Number of episodes to train: 62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a88f115596847f387462f2e718857d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Simulating:', max=10354, style=ProgressStyle(bar_color='maroon'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# ----------------------- INITIALIZE AND TRAIN MODEL ----------------------\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tql_model \u001b[38;5;241m=\u001b[39m CustomTabularQLearning(\n\u001b[1;32m     26\u001b[0m     env\u001b[38;5;241m=\u001b[39mtql_env,\n\u001b[1;32m     27\u001b[0m     loader\u001b[38;5;241m=\u001b[39mloader,\n\u001b[1;32m     28\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39mRANDOM_SEED,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtql_kwargs\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtql_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtql_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/citylearn/agents/base.py:140\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self, episodes, deterministic, deterministic_finish, logging_level)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m    139\u001b[0m     deterministic \u001b[38;5;241m=\u001b[39m deterministic \u001b[38;5;129;01mor\u001b[39;00m (deterministic_finish \u001b[38;5;129;01mand\u001b[39;00m episode \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m episodes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m     observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_time_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_tracker\u001b[38;5;241m.\u001b[39mepisode_time_steps\n\u001b[1;32m    142\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[ObsType, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[ObsType, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[ObsType, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/gym/core.py:379\u001b[0m, in \u001b[0;36mObservationWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(obs), info\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/gym/core.py:379\u001b[0m, in \u001b[0;36mObservationWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(obs), info\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# ----------------- CALCULATE NUMBER OF TRAINING EPISODES -----------------\n",
    "i = 3\n",
    "m = tql_env.observation_space[0].n\n",
    "n = tql_env.action_space[0].n\n",
    "t = tql_env.time_steps - 1\n",
    "tql_episodes = m*n*i/t\n",
    "tql_episodes = int(tql_episodes)\n",
    "print('Q-Table dimension:', (m, n))\n",
    "print('Number of episodes to train:', tql_episodes)\n",
    "\n",
    "# ------------------------------- SET LOADER ------------------------------\n",
    "loader = get_loader(max=tql_episodes*t)\n",
    "display(loader)\n",
    "\n",
    "# ----------------------- SET MODEL HYPERPARAMETERS -----------------------\n",
    "tql_kwargs = {\n",
    "    'epsilon': 1.0,\n",
    "    'minimum_epsilon': 0.01,\n",
    "    'epsilon_decay': 0.0001,\n",
    "    'learning_rate': 0.005,\n",
    "    'discount_factor': 0.99,\n",
    "}\n",
    "\n",
    "# ----------------------- INITIALIZE AND TRAIN MODEL ----------------------\n",
    "tql_model = CustomTabularQLearning(\n",
    "    env=tql_env,\n",
    "    loader=loader,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    **tql_kwargs\n",
    ")\n",
    "_ = tql_model.learn(episodes=tql_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jU_PUps73Gdm"
   },
   "source": [
    "We now evaluate the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uG900pOuiZH"
   },
   "outputs": [],
   "source": [
    "observations = tql_env.reset()\n",
    "\n",
    "while not tql_env.done:\n",
    "    actions = tql_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _ = tql_env.step(actions)\n",
    "\n",
    "# plot summary and compare with other control results\n",
    "plot_simulation_summary({'RBC': rbc_env, 'TQL': tql_env})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lpbit9J6uiZK"
   },
   "source": [
    "The figures plotted for the Tabular Q-Learning are compared against the baseline and your tuned RBC. The Q-Learning agent has performed worse than the baseline and RBC in terms of the building-level and district-level KPIs. The net electricity consumption profile as a result of the Q-Learning agent shows unstable and spiky consumption. The reason for this behavior is seen in the battery SoC curves where the changes in SoC are abrupt. This highlights an issue with our discretized action space having too large steps as trade off for maintaining a reasonably-sized Q-Table. We also see that the agent did not learn the unique day-to-day building needs as the SoC profile is identical every 24 time steps. This is a consequence of using a single observation, `hour` to learn.\n",
    "\n",
    "For the buildings 2 and 7 selected when the `RANDOM_SEED` = 0, we see that agent learned to charge the battery in building 2 in the early morning just after midnight and slightly charged and discharges during the day before completely depleting charge by midnight and into the early early hours if the next day. Building 7 on the other hand has 2 charge-discharge cycles each day that are split around noon.\n",
    "\n",
    "Since the Q-Table is 2 dimensional, we can visualize and spot the the state-action combinations that maximize the Q-value below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3OX9GNVnEDt"
   },
   "outputs": [],
   "source": [
    "def plot_table(\n",
    "    ax: plt.Axes, table: np.ndarray, title: str, cmap: str,\n",
    "    colorbar_label: str, xlabel: str, ylabel: str\n",
    ") -> plt.Axes:\n",
    "    \"\"\"Plot 2-dimensional table on a heat map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: plt.Axes\n",
    "        Figure axes\n",
    "    table: np.ndarray\n",
    "        Table array\n",
    "    title: str\n",
    "        axes title\n",
    "    cmap: str\n",
    "        Colormap\n",
    "    colorbar_label: str\n",
    "        Colorbar name\n",
    "    xlabel: str\n",
    "        Heat map x-axis label\n",
    "    ylabel: str\n",
    "        Heat map y-axis label\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax: plt.Axes\n",
    "        Plotted axes\n",
    "    \"\"\"\n",
    "\n",
    "    x = list(range(table.shape[0]))\n",
    "    y = list(range(table.shape[1]))\n",
    "    z = table.T\n",
    "    pcm = ax.pcolormesh(\n",
    "        x, y, z, shading='nearest', cmap=cmap,\n",
    "        edgecolors='black', linewidth=0.0\n",
    "    )\n",
    "    _ = fig.colorbar(\n",
    "        pcm, ax=ax, orientation='horizontal',\n",
    "        label=colorbar_label, fraction=0.025, pad=0.08\n",
    "    )\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdSMspcn3jFG"
   },
   "outputs": [],
   "source": [
    "cmap = 'coolwarm'\n",
    "figsize = (12, 8)\n",
    "fig, axs = plt.subplots(1, 3, figsize=figsize, sharey=True)\n",
    "axs[0] = plot_table(\n",
    "    axs[0], tql_model.q[0], 'Q-Table',\n",
    "    cmap, 'Q-Value', 'State (Hour)', 'Action Index'\n",
    ")\n",
    "axs[1] = plot_table(\n",
    "    axs[1], tql_model.q_exploration[0], 'Q-Table Exploration',\n",
    "    cmap, 'Count', 'State (Hour)', None\n",
    ")\n",
    "axs[2] = plot_table(\n",
    "    axs[2], tql_model.q_exploitation[0], 'Q-Table Exploitation',\n",
    "    cmap, 'Count', 'State (Hour)', None\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VAlscKD3iKO"
   },
   "source": [
    "The Q-Table (left) shows that for each hour, the Q-Values for most action indices are similar and very low (dark blue) asides the one action index that has been exploited. The middle heat map shows how many times each state-action pair was explored i.e. randomly chosen using $\\epsilon$, and we see that while most pairs have been visited at least once, some pairs have the monopoly. The figure on the right shows how many times state-action pairs were exploited. For each state, only one action was ever an exploitation candidate. This shows that the algorithm spent much time exploring randomly and the first discovered exploitation candidate for each state remained till learning was terminated. We can tell the exploration-exploitation balance through $\\epsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2Alb3JD3o6b"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Current Tabular Q-Learning epsilon after {tql_episodes}'\\\n",
    "        f' episodes and {tql_model.time_step} time steps:', tql_model.epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYmw5oz13pmy"
   },
   "source": [
    "Epsilon is still high so there is a higher probability of random exploration. The Q-Learning agent updates epsilon using the following exponential decay formula:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\textrm{max}(\\epsilon_{\\textrm{minimum}}, \\epsilon_{0} \\cdot e^{-\\epsilon_{\\textrm{decay}}*\\textrm{episode}})\n",
    "$$\n",
    "\n",
    "where $\\epsilon_{0}$ is $\\epsilon$ at time step 0. Thus with the current decay rate, $\\epsilon_{\\textrm{decay}}$ we can visualize the number of episodes needed to get to at least 50-50 probability of exploration-exploitation:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDjatcra3vPG"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n",
    "y = np.array([max(\n",
    "    tql_model.minimum_epsilon,\n",
    "    tql_model.epsilon_init*np.exp(-tql_model.epsilon_decay*e)\n",
    ") for e in range(100_000)])\n",
    "ref_x = len(y) - len(y[y <= 0.5]) - 1\n",
    "ref_y = y[ref_x]\n",
    "ax.plot(y)\n",
    "ax.axvline(ref_x, color='red', linestyle=':')\n",
    "ax.axhline(ref_y, color='red', linestyle=':')\n",
    "ax.axvline(tql_episodes, color='green', linestyle=':')\n",
    "ax.set_xlabel('Episode')\n",
    "text = f'{ref_x} training episodes needed to get\\nat least 50%'\\\n",
    "    ' exploitation probability.'\n",
    "ax.text(ref_x + 1000, ref_y + 0.05, text, color='red')\n",
    "ax.text(\n",
    "    tql_episodes + 1000,\n",
    "    ref_y - 0.1,\n",
    "    f'Current training episodes = {tql_episodes}',\n",
    "    va='bottom', color='green'\n",
    ")\n",
    "ax.set_ylabel(r'$\\epsilon$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUTniQTj3ycU"
   },
   "source": [
    "Now that we have experimented with the Tabular Q-Learning algorithm, what issues can you identify with this control approach? Ponder on these questions:\n",
    "\n",
    "1. How do the observations we use affect learning?\n",
    "2. How does the table dimension affect learning?\n",
    "4. What can we do to ensure that there is enough exploration of all state-action pairs?\n",
    "6. In what building control applications/examples could Tabular Q-Learning work well?\n",
    "7. In what building applications/examples will Tabular Q-Learning most likely fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNC34scp32zW"
   },
   "source": [
    "## Replacing the Q-Table with a Function Approximator\n",
    "\n",
    "Tabular Q-Learning is affected by the curse of dimensionality: as the size of the state space increases due to, e.g., continuous sensor inputs, the size of the Q-table has to necessarily increase is well. In particular for building control, the curse of dimensionality is significant, considering the potentially large number of sensors measuring various quantities (temperature, humidity, energy consumption, etc.) continuously. This means that the agent has an exponentially increasing number of state-action pairs to explore before it can converge to an optimal solution. Function approximators, e.g., linear regression or artificial neural networks ([Haykin (2009)](https://www.pearson.com/en-us/subject-catalog/p/neural-networks-and-learning-machines/P200000003278/9780133002553)), have been proposed as solutions that allow generalization by directly mapping the state-action pairs, $(s, a)$, to their respective Q-value, $Q(s, a)$. Refer to [Reinforcement learning for intelligent environments](https://www.taylorfrancis.com/chapters/edit/10.4324/9781315142074-37/reinforcement-learning-intelligent-environments-zoltan-nagy-june-young-park-jos-ramn-vzquez-canteli) for more information on how to make use of function approximators to improve learning in reinforcement learning control (RLC).\n",
    "\n",
    "In the next section, we will introduce the soft-actor critic (SAC) algorithm, which is a model-free Q-Learning algorithm, that uses a neural network to approximate the Q-values thus, reducing the cost of training compared to Tabular Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q74Y_l8a369T"
   },
   "source": [
    "# Optimize a Soft-Actor Critic Reinforcement Learning Controller\n",
    "---\n",
    "\n",
    "To control an environment like CityLearn that has continuous states and actions, tabular Q-learning is not practical, as it suffers from the _curse of dimensionality_. Actor-critic reinforcement learning (RL) methods use artificial neural networks to generalize across the state-action space. The actor network maps the current states to the actions that it estimates to be optimal. Then, the critic network evaluates those actions by mapping them, together with the states under which they were taken, to the Q-values.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://github.com/intelligent-environments-lab/CityLearn/blob/master/assets/images/sac_schematic.png?raw=true\"  width=\"350\" alt=\"SAC networks overview.\">\n",
    "  <figcaption>Figure: SAC networks overview (adopted from <a href=\"https://doi.org/10.1145/3408308.3427604\">Vazquez-Canteli et al., 2020</a>).</figcaption>\n",
    "</figure>\n",
    "\n",
    "Soft actor-critic (SAC) is a model-free off-policy RL algorithm. As an off-policy method, SAC can reuse experience and learn from fewer samples. SAC is based on three key elements: an actor-critic architecture, off-policy updates, and entropy maximization for efficient exploration and stable training. SAC learns three different functions: the actor (policy), the critic (soft Q-function), and the value function.\n",
    "\n",
    "This tutorial does not dive into the theory and algorithm of SAC but for interested participants please, refer to [Soft Actor-Critic Algorithms and Applications](https://doi.org/10.48550/arXiv.1812.05905).\n",
    "\n",
    "We will now initialize a new environment and plug it to an SAC agent to help us solve our control problem. Luckily, we do not have to write our own implementation of the SAC algorithm. Instead, we can make use of Python libraries that have standardized the implementation of a number of RL algorithms. One of such libraries that we will use is [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html). At the time of writing, there are [13 different RL algorithms](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html#rl-algorithms) implemented between Stable Baselines3 and Stable-Baselines3 - Contrib (contrib package for Stable-Baselines3 - experimental reinforcement learning code), including SAC.\n",
    "\n",
    "The new environment is initialized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "n9A8-38t390y"
   },
   "outputs": [],
   "source": [
    "sac_env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfKkwMPG4Aff"
   },
   "source": [
    "Before our environment is ready for use in Stable Baselines3, we need to take a couple of preprocessing steps in the form of wrappers. Firstly, we will wrap the environment using the `NormalizedObservationWrapper` (see [docs](https://www.citylearn.net/api/citylearn.wrappers.html#citylearn.wrappers.NormalizedObservationWrapper)) that ensure all observations that are served to the agent are [min-max normalized](https://www.codecademy.com/article/normalization) between [0, 1] and cyclical observations e.g. hour, are encoded using the [sine and cosine transformation](https://www.avanwyk.com/encoding-cyclical-features-for-deep-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "cBH83tFY4DhV"
   },
   "outputs": [],
   "source": [
    "sac_env = NormalizedObservationWrapper(sac_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYyxUFcO4Erv"
   },
   "source": [
    "Next, we wrap with the `StableBaselines3Wrapper` (see [docs](https://www.citylearn.net/api/citylearn.wrappers.html#citylearn.wrappers.StableBaselines3Wrapper)) that ensures observations, actions and rewards are served in manner that is compatible with Stable Baselines3 interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3Yq5edYr4JXo"
   },
   "outputs": [],
   "source": [
    "sac_env = StableBaselines3Wrapper(sac_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjx0GjAk4Kux"
   },
   "source": [
    "Now we can go ahead and initialize the SAC model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "WLkcTNQ34NLy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jt9744/.conda/envs/citylearn/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sac_model = SAC(policy='MlpPolicy', env=sac_env, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HQNnT8O4Qmi"
   },
   "source": [
    "In order to track the progress of learning, we will use a loader as we have done before. Stable Baselines3 makes use of callbacks to help with performing user-defined actions and procedures during learning. However, you do not need to know the specifics of the code below beyond being aware that it is used to update the loader value and store aggregated rewards at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "LtnL5S394TJB"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env: CityLearnEnv, loader: IntProgress):\n",
    "        r\"\"\"Initialize CustomCallback.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: Mapping[str, CityLearnEnv]\n",
    "            CityLearn environment instance.\n",
    "        loader: IntProgress\n",
    "            Progress bar.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(verbose=0)\n",
    "        self.loader = loader\n",
    "        self.env = env\n",
    "        self.reward_history = [0]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        r\"\"\"Called each time the env step function is called.\"\"\"\n",
    "\n",
    "        if self.env.time_step == 0:\n",
    "            self.reward_history.append(0)\n",
    "\n",
    "        else:\n",
    "            self.reward_history[-1] += sum(self.env.rewards[-1])\n",
    "\n",
    "        self.loader.value += 1\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLSppNHb4ViE"
   },
   "source": [
    "We will train the model for a fraction of the episodes we used to train the Tabular Q-Learning agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Hpytx_Rz4onF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of Tabular Q-Learning episodes used: 0.25\n",
      "Number of episodes to train: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf75f523e324929ad911ed4189c7f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Simulating:', max=2505, style=ProgressStyle(bar_color='maroon'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'CityLearnEnv' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ------------------------------- TRAIN MODEL -----------------------------\u001b[39;00m\n\u001b[1;32m     14\u001b[0m sac_callback \u001b[38;5;241m=\u001b[39m CustomCallback(env\u001b[38;5;241m=\u001b[39msac_env, loader\u001b[38;5;241m=\u001b[39msac_loader)\n\u001b[0;32m---> 15\u001b[0m sac_model \u001b[38;5;241m=\u001b[39m \u001b[43msac_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msac_total_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msac_callback\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:314\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[1;32m    307\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOffPolicyAlgorithm:\n\u001b[0;32m--> 314\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set the environment before calling learn()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:297\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, VectorizedActionNoise)\n\u001b[1;32m    294\u001b[0m ):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;241m=\u001b[39m VectorizedActionNoise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:424\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:78\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m     77\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m---> 78\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/shimmy/openai_gym_compatibility.py:233\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    (observation, info)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m(seed)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Options are ignored - https://github.com/openai/gym/blob/c755d5c35a25ab118746e2ba885894ff66fb8c43/gym/core.py\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: Wrapper.__getattr__ at line 241 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/citylearn/lib/python3.9/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CityLearnEnv' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "# ----------------- CALCULATE NUMBER OF TRAINING EPISODES -----------------\n",
    "fraction = 0.25\n",
    "sac_episodes = int(tql_episodes*fraction)\n",
    "print('Fraction of Tabular Q-Learning episodes used:', fraction)\n",
    "print('Number of episodes to train:', sac_episodes)\n",
    "sac_episode_timesteps = sac_env.time_steps - 1\n",
    "sac_total_timesteps = sac_episodes*sac_episode_timesteps\n",
    "\n",
    "# ------------------------------- SET LOADER ------------------------------\n",
    "sac_loader = get_loader(max=sac_total_timesteps)\n",
    "display(sac_loader)\n",
    "\n",
    "# ------------------------------- TRAIN MODEL -----------------------------\n",
    "sac_callback = CustomCallback(env=sac_env, loader=sac_loader)\n",
    "sac_model = sac_model.learn(\n",
    "    total_timesteps=sac_total_timesteps,\n",
    "    callback=sac_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErTZIqzS4zgO"
   },
   "source": [
    "With the SAC model trained, we will evaluate it for 1 episode using deterministic actions i.e. actions that maximized the Q-values during training as in the Tabular Q-Learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SxBBofg5pgL"
   },
   "outputs": [],
   "source": [
    "observations = sac_env.reset()\n",
    "sac_actions_list = []\n",
    "\n",
    "while not sac_env.done:\n",
    "    actions, _ = sac_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _ = sac_env.step(actions)\n",
    "    sac_actions_list.append(actions)\n",
    "\n",
    "# plot summary and compare with other control results\n",
    "plot_simulation_summary({'RBC': rbc_env, 'TQL': tql_env, 'SAC-1': sac_env})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tk-fLPpx5tZi"
   },
   "source": [
    "<img src=\"https://media.giphy.com/media/80TEu4wOBdPLG/giphy.gif\" height=200></img>\n",
    "\n",
    "The figures show that the SAC agent pretty much did not learn anything! The KPIs remain unchanged compared to the baseline and the battery SoCs are 0 all the time. What might be the case here? Let us have a look a the actions the SAC agent prescribed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLIKfCXO5xiD"
   },
   "outputs": [],
   "source": [
    "def plot_actions(actions_list: List[List[float]], title: str) -> plt.Figure:\n",
    "    \"\"\"Plots action time series for different buildings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actions_list: List[List[float]]\n",
    "        List of actions where each element with index, i,\n",
    "        in list is a list of the actions for different buildings\n",
    "        taken at time step i.\n",
    "    title: str\n",
    "        Plot axes title\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure with plotted axes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 1))\n",
    "    columns = [b.name for b in sac_env.buildings]\n",
    "    plot_data = pd.DataFrame(actions_list, columns=columns)\n",
    "    x = list(range(plot_data.shape[0]))\n",
    "\n",
    "    for c in plot_data.columns:\n",
    "        y = plot_data[c].tolist()\n",
    "        ax.plot(x, y, label=c)\n",
    "\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0)\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel(r'$\\frac{kWh}{kWh_{capacity}}$')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_actions(sac_actions_list, 'SAC-1 Actions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT5LI1iz51jd"
   },
   "source": [
    "<img src=\"https://media.giphy.com/media/b8RfbQFaOs1rO10ren/giphy.gif\" height=200></img>\n",
    "\n",
    "The SAC agent was calling for discharge all the time! To give it away, the reason for this behavior is the reward function that we have used to train the agent .\n",
    "\n",
    "Recall that the Bellman equation uses a reward, $r$, to update the Q-values hence the Q-Table is sensitive to the way the $r$ changes for $(s, a, s')$ tuple. That is to say, we need to make sure the reward we calculate after an action, $a$, is taken at state, $s$, quantifies how-well that action actually causes desirable next state, $s'$. If we define a poor reward function, we risk not learning quickly, or undesirable outcomes. See this example of the [implication of a poorly designed reward function](https://openai.com/research/faulty-reward-functions) where an agent learns to maximize a game score but with dangerous actions!\n",
    "\n",
    "The reward function is a variable in the CityLearn environment. The [docs](https://www.citylearn.net/api/citylearn.reward_function.html) provides information on in-built reward functions that can be used in simulation. The reward function used at run time is that which is defined in the schema and used to construct the environment. It can be overridden by parsing an alternative reward function that inherits from the `citylearn.reward_function.RewardFunction` class (see [docs](https://www.citylearn.net/api/citylearn.reward_function.html#citylearn.reward_function.RewardFunction)). Let us see what the current reward is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57tLgSp858N3"
   },
   "outputs": [],
   "source": [
    "help(sac_env.reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWprvdo46EYE"
   },
   "source": [
    "The current reward functions is the electricity consumption from the grid at the current time step returned as a negative value. While this reward will penalize high electricity consumption, it might not be ideal for all KPIs we are trying to optimize. As you would imagine, the best way to minimize electricity consumption is to try to move all loads to the battery hence, the insistence of the agent to continue to discharge the batteries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dt77XG7I6Fd6"
   },
   "source": [
    "## Defining a Custom Reward Function\n",
    "\n",
    "We want to reduce electricity consumption but also reduce its cost and emissions. Likewise, we want to reduce the peaks and ramping, and increase the load factor. One way to achieve this is to teach the agent to charge the batteries when electricity is cheap after 9 PM and before 4 PM, which typically coincides with when the grid is cleaner (lower emissions). But recall that each building is able to generate power provided there is solar radiation. So, we can take advantage of self-generation in the late morning to late afternoon to charge for free and discharge the rest of the day thus reducing electricity consumption, cost and emissions at the very least. Also, by shifting the early morning and evening peak loads to the batteries we can improve on our peak and load-factor KPIs.\n",
    "\n",
    "We should also teach our agent to ensure that renewable solar generation is not wasted by making use of the PV to charge the batteries while they are charged below capacity. On the flip side, the agent should learn to discharge when there is net positive grid load and the batteries still have stored energy.\n",
    "\n",
    "Given these learning objectives, we can now define a reward function that closely satisfies the criteria for which the agent will learn good rewards:\n",
    "\n",
    "$$\n",
    "    r = \\sum_{i=0}^n \\Big(p_i \\times |C_i|\\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p_i = -\\left(1 + \\textrm{sign}(C_i) \\times \\textrm{SOC}^{\\textrm{battery}}_i\\right)\n",
    "$$\n",
    "\n",
    "The reward function, $r$, is designed to minimize electricity cost, $C$. It is calculated for each building, $i$ and summed to provide the agent with a reward that is representative of all $n$ buildings. It encourages net-zero energy use by penalizing grid load satisfaction when there is energy in the battery as well as penalizing net export when the battery is not fully charged through the penalty term, $p$. There is neither penalty nor reward when the battery is fully charged during net export to the grid. Whereas, when the battery is charged to capacity and there is net import from the grid the penalty is maximized.\n",
    "\n",
    "Now we define this custom reward below and set it as the reward for the SAC agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPK08TkI6Jsi"
   },
   "outputs": [],
   "source": [
    "class CustomReward(RewardFunction):\n",
    "    def __init__(self, env_metadata: Mapping[str, Any]):\n",
    "        r\"\"\"Initialize CustomReward.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_metadata: Mapping[str, Any]:\n",
    "            General static information about the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(env_metadata)\n",
    "\n",
    "    def calculate(\n",
    "        self, observations: List[Mapping[str, int | float]]\n",
    "    ) -> List[float]:\n",
    "        r\"\"\"Returns reward for most recent action.\n",
    "\n",
    "        The reward is designed to minimize electricity cost.\n",
    "        It is calculated for each building, i and summed to provide the agent\n",
    "        with a reward that is representative of all n buildings.\n",
    "        It encourages net-zero energy use by penalizing grid load satisfaction\n",
    "        when there is energy in the battery as well as penalizing\n",
    "        net export when the battery is not fully charged through the penalty\n",
    "        term. There is neither penalty nor reward when the battery\n",
    "        is fully charged during net export to the grid. Whereas, when the\n",
    "        battery is charged to capacity and there is net import from the\n",
    "        grid the penalty is maximized.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations: List[Mapping[str, int | float]]\n",
    "            List of all building observations at current\n",
    "            :py:attr:`citylearn.citylearn.CityLearnEnv.time_step`\n",
    "            that are got from calling\n",
    "            :py:meth:`citylearn.building.Building.observations`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward: List[float]\n",
    "            Reward for transition to current timestep.\n",
    "        \"\"\"\n",
    "\n",
    "        reward_list = []\n",
    "\n",
    "        for o, m in zip(observations, self.env_metadata['buildings']):\n",
    "            cost = o['net_electricity_consumption']*o['electricity_pricing']\n",
    "            battery_capacity = m['electrical_storage']['capacity']\n",
    "            battery_soc = o.get('electrical_storage_soc', 0.0)\n",
    "            penalty = -(1.0 + np.sign(cost)*battery_soc)\n",
    "            reward = penalty*abs(cost)\n",
    "            reward_list.append(reward)\n",
    "\n",
    "        reward = [sum(reward_list)]\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lolPaXje6Mlk"
   },
   "source": [
    "Let us repeat all the previous steps we took in the former SAC simulation where the only difference in the workflow here is the use of our new custom reward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38i9ZAnp6Ns7"
   },
   "outputs": [],
   "source": [
    "# ----------------- INITIALIZE ENVIRONMENT -----------------\n",
    "sacr_env = CityLearnEnv(schema)\n",
    "\n",
    "# -------------------- SET CUSTOM REWARD -------------------\n",
    "sacr_env.reward_function = CustomReward(sacr_env.get_metadata())\n",
    "\n",
    "# -------------------- WRAP ENVIRONMENT --------------------\n",
    "sacr_env = NormalizedObservationWrapper(sacr_env)\n",
    "sacr_env = StableBaselines3Wrapper(sacr_env)\n",
    "\n",
    "# -------------------- INITIALIZE AGENT --------------------\n",
    "sacr_model = SAC(policy='MlpPolicy', env=sacr_env, seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ----------------------- SET LOADER -----------------------\n",
    "print('Number of episodes to train:', sac_episodes)\n",
    "sac_modr_loader = get_loader(max=sac_total_timesteps)\n",
    "display(sac_modr_loader)\n",
    "\n",
    "# ----------------------- TRAIN AGENT ----------------------\n",
    "sacr_callback = CustomCallback(env=sacr_env, loader=sac_modr_loader)\n",
    "sacr_model = sacr_model.learn(\n",
    "    total_timesteps=sac_total_timesteps,\n",
    "    callback=sacr_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsMNFhJX6RWb"
   },
   "source": [
    "Finally, evaluate the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9uB1Fr56TmN"
   },
   "outputs": [],
   "source": [
    "observations = sacr_env.reset()\n",
    "sacr_actions_list = []\n",
    "\n",
    "while not sacr_env.done:\n",
    "    actions, _ = sacr_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _ = sacr_env.step(actions)\n",
    "    sacr_actions_list.append(actions)\n",
    "\n",
    "plot_simulation_summary(\n",
    "    {'RBC': rbc_env, 'TQL': tql_env, 'SAC-1': sac_env, 'SAC-2': sacr_env}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46jpY4Or6X4I"
   },
   "source": [
    "Finally, we have results that have improved the baseline KPIs all thanks to our custom reward function! The agent has learned to take advantage of the solar generation to charge the batteries and discharge the stored energy during the evening peak.\n",
    "\n",
    "Let us now have a look at the actions that the agent predicted in the deterministic simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0s2C5gOf6aSO"
   },
   "outputs": [],
   "source": [
    "fig = plot_actions(sacr_actions_list, 'SAC Actions using Custom Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geSmAWJ76ePi"
   },
   "source": [
    "The agent learned the different building needs as building 7 begins to charge later than building 2 daily (selected buildings when `RANDOM_SEED` = 0). The agent discharges the batteries differently as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJniwtkc6gz9"
   },
   "source": [
    "## Evaluate the Episode Rewards for RL Algorithms\n",
    "\n",
    "We can also investigate the convergence rate in training by looking at the sum of rewards in each episode. We expect to see the reward sum increase as we train on more episodes and eventually plateau when exploitation increases or performance can not be further improved. We will look at the reward trajectory for the Tabular Q-Learning, SAC with and without custom reward models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D-Qlfv-6kNI"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ax: plt.Axes, rewards: List[float], title: str) -> plt.Axes:\n",
    "    \"\"\"Plots rewards over training episodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards: List[float]\n",
    "        List of reward sum per episode.\n",
    "    title: str\n",
    "        Plot axes title\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax: plt.Axes\n",
    "        Plotted axes\n",
    "    \"\"\"\n",
    "\n",
    "    ax.plot(rewards)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZOaEMwQ6lqa"
   },
   "outputs": [],
   "source": [
    "rewards = {\n",
    "    'Tabular Q-Learning': tql_model.reward_history[:tql_episodes],\n",
    "    'SAC-1': sac_callback.reward_history[:sac_episodes],\n",
    "    'SAC-2': sacr_callback.reward_history[:sac_episodes]\n",
    "}\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 2))\n",
    "\n",
    "for ax, (k, v) in zip(fig.axes, rewards.items()):\n",
    "    ax = plot_rewards(ax, v, k)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ451oYn6qEI"
   },
   "source": [
    "Some questions to ponder on:\n",
    "1. What do you notice in the reward trajectories for the three models?\n",
    "2. Which model converged?\n",
    "3. Which model did not learn anything?\n",
    "4. Which model needs to train some more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EtYMAY-6s7o"
   },
   "source": [
    "# Tune your SAC Agent\n",
    "---\n",
    "\n",
    "Thus far, you have learned to manage battery charge/discharge for a district of buildings by 1) tuning your own rule-based control (RBC) agent, 2) training a Tabular Q-Learning agent, 3) implementing the soft-actor critic (SAC) off-policy reinforcement learning (RL) algorithm with and crude reward function and a better tailored reward function.\n",
    "\n",
    "When each control agent is evaluated on the a set of building-level and district-level KPIs, we find that if carefully tuned, your RBC will improve the baseline albeit a painstaking effort. The Tabular Q-Learning agent has the potential to adapt to unique building properties but suffers from the curse of dimensionality affecting its convergence to an optimal solution for the battery management. We also find that the SAC agent is sensitive to the reward function design and with a custom reward that is tailored towards achieving our evaluation KPIs, we can achieve a performance that is better than the baseline case and potentially better than an averagely tuned RBC.\n",
    "\n",
    "However, we find that the SAC + custom reward case did not converge after our set number of training episodes. Also, the improvements it provides beyond the baseline are not very large. Hence, there is still room for improvement.\n",
    "\n",
    "In the next cells, you will improve the SAC model by:\n",
    "1. Revising the custom reward function with a function you deem more appropriate towards optimizing the KPIs. Perhaps, you can design a reward function that targets a specific KPI. You can also keep the current custom reward function.\n",
    "2. Changing the length of training i.e. episodes.\n",
    "3. Optimizing the SAC hyperparameters. In our previous models, we used the default Stable Baselines3 hyperparameters. Hyperparameter tuning is an _art_ of its own. Refer to the [Stable Baselines3 SAC docs](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html#stable_baselines3.sac.SAC) to learn about the SAC hyperparameters. Also, refer to [Training with Soft-Actor Critic](https://github.com/yosider/ml-agents-1/blob/master/docs/Training-SAC.md#training-with-soft-actor-critic) for a more elaborate description of what the hyperparameters mean, their typical values and appropriate values for different case scenarios.\n",
    "\n",
    "You may also choose to update the active observations. Recall that thus far, we have only used the `hour` observation. Refer to the [CityLearn Observation docs](https://www.citylearn.net/overview/observations.html) to discover other available environment observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nunxV4ev6wWb"
   },
   "source": [
    "## Set Environment, Agent and Reward Function\n",
    "\n",
    "The next cell is a __recipe__ for your tuned SAC and custom environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjkvrnpS6yhL"
   },
   "outputs": [],
   "source": [
    "# -------------------- CUSTOMIZE **YOUR** ENVIRONMENT --------------------\n",
    "# Include other observations if needed.\n",
    "# See https://www.citylearn.net/overview/observations.html\n",
    "# for table of observations that you can include\n",
    "# NOTE: More active observations could mean longer trainer time.\n",
    "your_active_observations = [\n",
    "    'hour',\n",
    "    # 'day_type'\n",
    "]\n",
    "\n",
    "# ------------------ SET **YOUR** AGENT HYPERPARAMETERS ------------------\n",
    "# try out different hyperparameter value combinations to see\n",
    "# which one provides you with the best KPIs. See\n",
    "# https://github.com/yosider/ml-agents-1/blob/master/docs/Training-SAC.md#training-with-soft-actor-critic\n",
    "# for a guide on how to select hyperparameter values.\n",
    "your_agent_kwargs = {\n",
    "    'learning_rate': 0.0003,\n",
    "    'buffer_size': 1000000,\n",
    "    'learning_starts': 100,\n",
    "    'batch_size': 256,\n",
    "    'tau': 0.005,\n",
    "    'gamma': 0.99,\n",
    "    'train_freq': 1,\n",
    "}\n",
    "\n",
    "# --------------- SET **YOUR** NUMBER OF TRAINING EPISODES ---------------\n",
    "your_episodes = sac_episodes\n",
    "\n",
    "# --------------- DEFINE **YOUR** CUSTOM REWARD FUNCTION -----------------\n",
    "class YourCustomReward(CustomReward):\n",
    "    def __init__(self, env_metadata: Mapping[str, Any]):\n",
    "        r\"\"\"Initialize CustomReward.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_metadata: Mapping[str, Any]:\n",
    "            General static information about the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(env_metadata)\n",
    "\n",
    "    def calculate(\n",
    "        self, observations: List[Mapping[str, int | float]]\n",
    "    ) -> List[float]:\n",
    "        r\"\"\"Returns reward for most recent action.\n",
    "\n",
    "        <Provide a description for your custom reward>.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations: List[Mapping[str, Union[int, float]]]\n",
    "            List of all building observations at current\n",
    "            :py:attr:`citylearn.citylearn.CityLearnEnv.time_step`\n",
    "            that are got from calling\n",
    "            :py:meth:`citylearn.building.Building.observations`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward: List[float]\n",
    "            Reward for transition to current timestep.\n",
    "        \"\"\"\n",
    "\n",
    "        # comment the next line of code and define your custom reward otherwise,\n",
    "        # leave as is to use the previously defined custom reward function.\n",
    "        reward = super().calculate(observations)\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M212veso630o"
   },
   "source": [
    "## Train\n",
    "\n",
    "Here we define one function that performs all the procedures we took to train the SAC agent from selecting buildings, simulation period and active observations to initializing and wrapping the environment, initializing the agent, training it a nd reporting it's results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJc3Rvsf668Y"
   },
   "outputs": [],
   "source": [
    "def train_your_custom_sac(\n",
    "    agent_kwargs: dict, episodes: int, reward_function: RewardFunction,\n",
    "    building_count: int, day_count: int, active_observations: List[str],\n",
    "    random_seed: int, reference_envs: Mapping[str, CityLearnEnv] = None,\n",
    "    show_figures: bool = None\n",
    ") -> dict:\n",
    "    \"\"\"Trains a custom soft-actor critic (SAC) agent on a custom environment.\n",
    "\n",
    "    Trains an SAC agent using a custom environment and agent hyperparamter\n",
    "    setup and plots the key performance indicators (KPIs), actions and\n",
    "    rewards from training and evaluating the agent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent_kwargs: dict\n",
    "        Defines the hyperparameters used to initialize the SAC agent.\n",
    "    episodes: int\n",
    "        Number of episodes to train the agent for.\n",
    "    reward_function: RewardFunction\n",
    "        A base or custom reward function class.\n",
    "    building_count: int\n",
    "        Number of buildings to set as active in schema.\n",
    "    day_count: int\n",
    "        Number of simulation days.\n",
    "    active_observations: List[str]\n",
    "        Names of observations to set active to be passed to control agent.\n",
    "    random_seed: int\n",
    "        Seed for pseudo-random number generator.\n",
    "    reference_envs: Mapping[str, CityLearnEnv], default: None\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "    show_figures: bool, default: False\n",
    "        Indicate if summary figures should be plotted at the end of\n",
    "        evaluation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: dict\n",
    "        Results from training the agent as well as some input variables\n",
    "        for reference including the following value keys:\n",
    "\n",
    "            * random_seed: int\n",
    "            * env: CityLearnEnv\n",
    "            * model: SAC\n",
    "            * actions: List[float]\n",
    "            * rewards: List[float]\n",
    "            * agent_kwargs: dict\n",
    "            * episodes: int\n",
    "            * reward_function: RewardFunction\n",
    "            * buildings: List[str]\n",
    "            * simulation_start_time_step: int\n",
    "            * simulation_end_time_step: int\n",
    "            * active_observations: List[str]\n",
    "            * train_start_timestamp: datetime\n",
    "            * train_end_timestamp: datetime\n",
    "    \"\"\"\n",
    "\n",
    "    # get schema\n",
    "    schema = DataSet.get_schema('citylearn_challenge_2022_phase_all')\n",
    "\n",
    "    # select buildings\n",
    "    schema, buildings = set_schema_buildings(\n",
    "        schema, building_count, random_seed\n",
    "    )\n",
    "    print('Selected buildings:', buildings)\n",
    "\n",
    "    # select days\n",
    "    schema, simulation_start_time_step, simulation_end_time_step =\\\n",
    "        set_schema_simulation_period(schema, day_count, random_seed)\n",
    "    print(\n",
    "        f'Selected {day_count}-day period time steps:',\n",
    "        (simulation_start_time_step, simulation_end_time_step)\n",
    "    )\n",
    "\n",
    "    # set active observations\n",
    "    schema = set_active_observations(schema, active_observations)\n",
    "    print(f'Active observations:', active_observations)\n",
    "\n",
    "    # initialize environment\n",
    "    env = CityLearnEnv(schema, central_agent=True)\n",
    "\n",
    "    # set reward function\n",
    "    env.reward_function = reward_function(env.get_metadata())\n",
    "\n",
    "    # wrap environment\n",
    "    env = NormalizedObservationWrapper(env)\n",
    "    env = StableBaselines3Wrapper(env)\n",
    "\n",
    "    # initialize agent\n",
    "    model = SAC('MlpPolicy', env, **agent_kwargs, seed=random_seed)\n",
    "\n",
    "    # initialize loader\n",
    "    total_timesteps = episodes*(env.time_steps - 1)\n",
    "    print('Number of episodes to train:', episodes)\n",
    "    loader = get_loader(max=total_timesteps)\n",
    "    display(loader)\n",
    "\n",
    "    # initialize callback\n",
    "    callback = CustomCallback(env=env, loader=loader)\n",
    "\n",
    "    # train agent\n",
    "    train_start_timestamp = datetime.utcnow()\n",
    "    model = model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "    train_end_timestamp = datetime.utcnow()\n",
    "\n",
    "    # evaluate agent\n",
    "    observations = env.reset()\n",
    "    actions_list = []\n",
    "\n",
    "    while not env.done:\n",
    "        actions, _ = model.predict(observations, deterministic=True)\n",
    "        observations, _, _, _ = env.step(actions)\n",
    "        actions_list.append(actions)\n",
    "\n",
    "    # get rewards\n",
    "    rewards = callback.reward_history[:episodes]\n",
    "\n",
    "    # plot summary and compare with other control results\n",
    "    if show_figures is not None and show_figures:\n",
    "        env_id = 'Your-SAC'\n",
    "\n",
    "        if reference_envs is None:\n",
    "            reference_envs = {env_id: env}\n",
    "        else:\n",
    "            reference_envs = {env_id: env, **reference_envs}\n",
    "\n",
    "        plot_simulation_summary(reference_envs)\n",
    "\n",
    "        # plot actions\n",
    "        plot_actions(actions_list, f'{env_id} Actions')\n",
    "\n",
    "        # plot rewards\n",
    "        _, ax = plt.subplots(1, 1, figsize=(5, 2))\n",
    "        ax = plot_rewards(ax, rewards, f'{env_id} Rewards')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        'random_seed': random_seed,\n",
    "        'env': env,\n",
    "        'model': model,\n",
    "        'actions': actions_list,\n",
    "        'rewards': rewards,\n",
    "        'agent_kwargs': agent_kwargs,\n",
    "        'episodes': episodes,\n",
    "        'reward_function': reward_function,\n",
    "        'buildings': buildings,\n",
    "        'simulation_start_time_step': simulation_start_time_step,\n",
    "        'simulation_end_time_step': simulation_end_time_step,\n",
    "        'active_observations': active_observations,\n",
    "        'train_start_timestamp': train_start_timestamp,\n",
    "        'train_end_timestamp': train_end_timestamp,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga5ExjjdHkRx"
   },
   "source": [
    "Now, we shall train!!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMWU5NzcxNGQzODRiYmI0MzQwNDVlYWU1NjhjODI0ZDhhZDhlNzM3NCZjdD1n/KGYmNdjOUxkFO8JVbM/giphy.gif\" height=200></img>\n",
    "\n",
    "Note that you can use a for loop to train on different `agent_kwargs` combinations in order to find which hyperparameters give the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFLBpXVA7HQ2"
   },
   "outputs": [],
   "source": [
    "your_results = train_your_custom_sac(\n",
    "    agent_kwargs=your_agent_kwargs,\n",
    "    episodes=your_episodes,\n",
    "    reward_function=YourCustomReward,\n",
    "    building_count=BUILDING_COUNT,\n",
    "    day_count=DAY_COUNT,\n",
    "    active_observations=your_active_observations,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    reference_envs={\n",
    "        'RBC': rbc_env,\n",
    "        # 'TQL': tql_env,\n",
    "        'SAC-1': sac_env,\n",
    "        'SAC-2': sacr_env\n",
    "    },\n",
    "    show_figures=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JC5e_3AW8a0w"
   },
   "source": [
    "## Submit\n",
    "\n",
    "You may choose to submit __your results__ to the [scoreboard](https://docs.google.com/spreadsheets/d/1wI1mz7fFiNNc1eZvZfKu_Id23y3QAzL_joVmiqUHm2U/edit?resourcekey#gid=939604299). To this we will programmatically submit your results to a Google Form that live updates the scoreboard in a Google Sheet.\n",
    "\n",
    "Run the following cell to set the function that helps us with the submission.\n",
    "\n",
    ">  **NOTE**:\n",
    "> You do not need to understand the content of the next code cell where the result submission function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5xzZ4nt8bRL"
   },
   "outputs": [],
   "source": [
    "def post_results(tag: str, results: dict) -> Tuple[dict, requests.Response]:\n",
    "    \"\"\"Submit your trained SAC model results to public scoreboard.\n",
    "\n",
    "    Submits trained SAC model results to a Google Form and results\n",
    "    are displayed and ranked in Google Sheets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag: str\n",
    "        A name to use to identify submitted results in scoreboard.\n",
    "        Avoid including personal identifiers in the tag.\n",
    "    results: dict\n",
    "        Mapping of results from your simulation. It is the variable returned\n",
    "        by the :code:`train_your_custom_sac` function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    payload: dict\n",
    "        Submitted results.\n",
    "    response: requests.Response\n",
    "        Form post request response.\n",
    "    \"\"\"\n",
    "\n",
    "    # submission for ID\n",
    "    form_id = '1FAIpQLSc69VR3t5z7ag6ydvv11mDpdBS8ruhz4yBfWD_81IUZ2IYtEw'\n",
    "\n",
    "    # url to get and fill the form\n",
    "    get_url = f'https://docs.google.com/forms/d/e/{form_id}/viewform?usp=sf_link'\n",
    "\n",
    "    # url to submit the form\n",
    "    post_url = f'https://docs.google.com/forms/u/1/d/e/{form_id}/formResponse'\n",
    "\n",
    "    # get KPIs\n",
    "    kpis = get_kpis(results['env']).pivot(\n",
    "        index='kpi', columns='name', values='value'\n",
    "    ).to_dict()\n",
    "    kpis = {k: {\n",
    "        k_: float(v_) for k_, v_ in v.items() if not math.isnan(v_)\n",
    "    } for k, v in kpis.items()}\n",
    "\n",
    "    # set payload\n",
    "    datetime_fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    buildings = [int(b.split('_')[-1]) for b in results['buildings']]\n",
    "    buildings = sorted(buildings)\n",
    "    buildings = ', '.join([str(b) for b in buildings])\n",
    "    payload = {\n",
    "        'uid': uuid.uuid4().hex,\n",
    "        'create_timestamp': datetime.utcnow().strftime(datetime_fmt),\n",
    "        'train_start_timestamp': results['train_start_timestamp'].strftime(datetime_fmt),\n",
    "        'train_end_timestamp': results['train_end_timestamp'].strftime(datetime_fmt),\n",
    "        'tag': '' if tag is None else tag,\n",
    "        'random_seed': results['random_seed'],\n",
    "        'buildings': buildings,\n",
    "        'simulation_start_time_step': int(results['simulation_start_time_step']),\n",
    "        'simulation_end_time_step': int(results['simulation_end_time_step']),\n",
    "        'episodes': results['episodes'],\n",
    "        'active_observations': ', '.join(sorted(results['active_observations'])),\n",
    "        'agent_name': str(results['model'].__class__),\n",
    "        'agent_kwargs': results['agent_kwargs'],\n",
    "        'reward_function_calculate': inspect.getsource(results['reward_function'].calculate),\n",
    "        'kpis': kpis,\n",
    "        'district_electricity_consumption': kpis['District']['Consumption'],\n",
    "        'district_cost': kpis['District']['Cost'],\n",
    "        'district_carbon_emissions': kpis['District']['Emissions'],\n",
    "        'district_ramping': kpis['District']['Ramping'],\n",
    "        'district_average_daily_peak': kpis['District']['Avg. daily peak'],\n",
    "        'district_load_factor': kpis['District']['1 - load factor'],\n",
    "    }\n",
    "\n",
    "    # get form question IDs\n",
    "    response = requests.get(get_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pattern = re.compile('var FB_PUBLIC_LOAD_DATA_ = (.*?);')\n",
    "    string = soup.findAll(\n",
    "        'script', string=pattern\n",
    "    )[0].string.split(' = ')[-1][:-1]\n",
    "    questions = json.loads(string)[1][1]\n",
    "    questions = {q[1]: q[4][0][0] for q in questions}\n",
    "\n",
    "    # set form question answers\n",
    "    payload = {k: json.dumps(payload[k]) for k, v in questions.items()}\n",
    "    parsed_payload = {f'entry.{questions[k]}': v for k, v in payload.items()}\n",
    "\n",
    "    # submit form\n",
    "    response = requests.post(post_url, data=parsed_payload)\n",
    "\n",
    "    return payload, response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljADPiVk8j1U"
   },
   "source": [
    "Finally, run the following cell to set up the submission interface.\n",
    "\n",
    ">  **NOTE**:\n",
    "> You do not need to understand the content of the next code cell where the result submission user interface is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqSZRoZ48lJg"
   },
   "outputs": [],
   "source": [
    "# instructions\n",
    "instructions = \"\"\"\n",
    "<h1>Submit your Results</h1>\n",
    "<p>Use this interactive widget to submit the results of your tuned SAC\n",
    "agent!</p>\n",
    "\n",
    "<p style=\"color:yellow\"><strong>NOTE:</strong> The scoreboard\n",
    "is merely an informational tool. Please, we urge participants\n",
    "to adhere to fair use practices including but not limited to:\n",
    "\n",
    "<ul style=\"color:yellow\">\n",
    "    <li>Do not spam the scoreboard.</li>\n",
    "    <li>Make only one submission for every custom agent\n",
    "    and environment set up.</li>\n",
    "    <li>Do not make alterations to the\n",
    "    <code>post_results</code> function.</li>\n",
    "</ul>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p>Your results are displayed on the\n",
    "<a href=\"https://docs.google.com/spreadsheets\n",
    "/d/1wI1mz7fFiNNc1eZvZfKu_Id23y3QAzL_joVmiqUHm2U/\n",
    "edit?resourcekey#gid=939604299\" target=\"_blank\">scoreboard</a>.</p>\n",
    "\n",
    "\n",
    "<p><strong>Provide a tag (avoid personal identifiers)\n",
    "for your submission and hit the <strong>Submit</strong> button:</strong></p>\n",
    "\"\"\"\n",
    "instructions_html_ui = HTML(value=instructions, placeholder='Instructions')\n",
    "\n",
    "\n",
    "# tag text input\n",
    "tag_text_ui = Text(\n",
    "    value='',\n",
    "    placeholder='Provide a submission tag',\n",
    "    description='Tag:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# submit button\n",
    "submit_button_ui = Button(\n",
    "    description='Submit',\n",
    "    disabled=True,\n",
    "    button_style='success',\n",
    "    tooltip='Submit your Results',\n",
    "    icon='check'\n",
    ")\n",
    "interactions_ui = HBox([tag_text_ui, submit_button_ui])\n",
    "\n",
    "# post-submission html\n",
    "post_submission_html_ui = HTML(value='', placeholder='Post submission report')\n",
    "\n",
    "def on_tag_value_change(change):\n",
    "    \"\"\"Activate/deactivate submit button based on tag value.\"\"\"\n",
    "\n",
    "    value = tag_text_ui.value.strip(' ')\n",
    "\n",
    "    if len(value) > 0:\n",
    "        submit_button_ui.disabled = False\n",
    "    else:\n",
    "        submit_button_ui.disabled = True\n",
    "\n",
    "def on_submit_button_ui_clicked(b):\n",
    "    \"\"\"Submit your results when submit button is clicked.\"\"\"\n",
    "\n",
    "    # set UI pre-submission states\n",
    "    tag_text_ui.disabled = True\n",
    "    submit_button_ui.disabled = True\n",
    "    current_submit_button_description = submit_button_ui.description\n",
    "    submit_button_ui.description = 'Submitting ...'\n",
    "    tag = tag_text_ui.value.strip()\n",
    "    post_submission_html_ui.value = ''\n",
    "\n",
    "    # make submission\n",
    "    payload, response = post_results(tag, your_results)\n",
    "\n",
    "    # confirm successful submission\n",
    "    try:\n",
    "        assert response.status_code == 200\n",
    "        assert 'Your response has been recorded' in response.text\n",
    "        post_submission_html = f\"\"\"\n",
    "        <p style=\"color:green\">Your last submission\n",
    "        on \"{payload['create_timestamp'].strip('\"')} UTC\"\n",
    "        with tag: {payload['tag']}\n",
    "        and unique ID: {payload['uid']}\n",
    "        was successful!</p>\n",
    "        \"\"\"\n",
    "\n",
    "    except AssertionError:\n",
    "        post_submission_html = f\"\"\"\n",
    "        <p style=\"color:red\">Your last submission\n",
    "        on \"{payload['create_timestamp'].strip('\"')} UTC\"\n",
    "        with tag: {payload['tag']}\n",
    "        was unsuccessful!</p>\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    # set UI post-submission states\n",
    "    submit_button_ui.description = current_submit_button_description\n",
    "    tag_text_ui.value = ''\n",
    "    tag_text_ui.disabled = False\n",
    "    submit_button_ui.disabled = False\n",
    "    post_submission_html_ui.value = post_submission_html\n",
    "\n",
    "\n",
    "# callbacks\n",
    "tag_text_ui.observe(on_tag_value_change, names='value')\n",
    "submit_button_ui.on_click(on_submit_button_ui_clicked)\n",
    "\n",
    "# show UI\n",
    "ui = VBox([instructions_html_ui, interactions_ui, post_submission_html_ui])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhkmytKNU_Z2"
   },
   "source": [
    "# Next Steps\n",
    "---\n",
    "\n",
    "Now that you are a _CityLearner_, here are some next steps and ideas (asides the awesome ideas you probably already have of course ):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PEwzcNdLYsK"
   },
   "source": [
    "## Other Ideas\n",
    "\n",
    "- Rerun the entire tutorial with a new [RANDOM_SEED](#scrollTo=vfnO0QBszXcS&line=1&uniqifier=1), [number of buildings](#scrollTo=6C6S46xmz50t&line=2&uniqifier=1) (between 1 - 15), [number of days](#scrollTo=6C6S46xmz50t&line=5&uniqifier=1) (1 - 365) and/or [observations](#scrollTo=6C6S46xmz50t&line=8&uniqifier=1). Remember to [set the number of discretization bins](#scrollTo=_6HotiSW4Pe8&line=2&uniqifier=1) for Tabular Q-Learning if you use other observations in your simulations.\n",
    "- How does the Tabular Q-Learning agent perform with a different set of hyperparameters and/or active observations?\n",
    "- How well does the Tabular Q-Learning learn if we use the custom reward function we defined? Are there any improvements compared to the original reward function?\n",
    "- Try to train the SAC agent on all the buildings and the full one-year period in the `citylearn_challenge_2022_phase_all` dataset.\n",
    "- Can you still improve some KPIs without self-generation in the buildings i.e. no photovoltaic (PV) system?\n",
    "- In our hand-on experiments here, we trained and tested on the same days. In reality, when an RL agent is deployed, it may experience states and state transitions that were not seen during training. Try to evaluate your trained agent on a different sequence of days and see if your trained agent generalizes well.\n",
    "- Try out the other datasets in CityLearn.\n",
    "- Make a submission to previous and any current [The CityLearn Challenge editions](https://www.citylearn.net/citylearn_challenge/index.html).\n",
    "- Bring your own dataset to CityLearn!\n",
    "- \\<Insert __YOUR__ ideas \\>\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/3ohs86vZAWiJXWvQI0/giphy.gif\" height=200></img>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "citylearn [~/.conda/envs/citylearn/]",
   "language": "python",
   "name": "conda_citylearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "aef885a76ce31739e452d1e6967b400907b14827afd25732d0e38ec88d4e0d05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
